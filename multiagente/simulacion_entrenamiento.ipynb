{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 1 (CORREGIDA): INSTALACIÓN Y CREACIÓN DE ESCENARIOS\n",
        "# ===================================================================\n",
        "import os\n",
        "\n",
        "# --- 1.1 INSTALACIÓN DE DEPENDENCIAS ---\n",
        "print(\"⚙️ Instalando SUMO y herramientas...\")\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo add-apt-repository -y ppa:sumo/stable > /dev/null\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo apt-get install -y sumo sumo-tools sumo-doc ffmpeg > /dev/null\n",
        "os.environ['SUMO_HOME'] = '/usr/share/sumo'\n",
        "os.environ['LIBSUMO_AS_TRACI'] = '1'\n",
        "print(\"✅ SUMO y herramientas de video instalados y configurados.\")\n",
        "\n",
        "# --- CORRECCIÓN AQUÍ ---\n",
        "print(\"\\n🐍 Instalando bibliotecas de Python...\")\n",
        "!pip install -q torch pandas numpy traci\n",
        "print(\"✅ Bibliotecas de Python (incluyendo traci) instaladas.\")\n",
        "\n",
        "\n",
        "# --- 1.2 CREACIÓN DEL ESCENARIO GRID 2x2 ---\n",
        "print(\"\\n📂 Creando escenario de red 2x2...\")\n",
        "os.makedirs('data', exist_ok=True) # Creamos la carpeta 'data' donde vivirá el mapa\n",
        "\n",
        "# Nodos\n",
        "with open(\"data/2_2_intersections.nod.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<nodes>\n",
        "    <node id=\"C11\" x=\"0\" y=\"100\" type=\"traffic_light\"/> <node id=\"C12\" x=\"100\" y=\"100\" type=\"traffic_light\"/>\n",
        "    <node id=\"C21\" x=\"0\" y=\"0\" type=\"traffic_light\"/>   <node id=\"C22\" x=\"100\" y=\"0\" type=\"traffic_light\"/>\n",
        "    <node id=\"N1\" x=\"0\" y=\"200\"/> <node id=\"N2\" x=\"100\" y=\"200\"/>\n",
        "    <node id=\"S1\" x=\"0\" y=\"-100\"/> <node id=\"S2\" x=\"100\" y=\"-100\"/>\n",
        "    <node id=\"W1\" x=\"-100\" y=\"100\"/> <node id=\"W2\" x=\"-100\" y=\"0\"/>\n",
        "    <node id=\"E1\" x=\"200\" y=\"100\"/> <node id=\"E2\" x=\"200\" y=\"0\"/>\n",
        "</nodes>\"\"\")\n",
        "\n",
        "# Aristas\n",
        "with open(\"data/2_2_intersections.edg.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<edges>\n",
        "    <edge from=\"W1\" to=\"C11\" id=\"W1_C11\"/> <edge from=\"C11\" to=\"W1\" id=\"C11_W1\"/>\n",
        "    <edge from=\"W2\" to=\"C21\" id=\"W2_C21\"/> <edge from=\"C21\" to=\"W2\" id=\"C21_W2\"/>\n",
        "    <edge from=\"E1\" to=\"C12\" id=\"E1_C12\"/> <edge from=\"C12\" to=\"E1\" id=\"C12_E1\"/>\n",
        "    <edge from=\"E2\" to=\"C22\" id=\"E2_C22\"/> <edge from=\"C22\" to=\"E2\" id=\"C22_E2\"/>\n",
        "    <edge from=\"N1\" to=\"C11\" id=\"N1_C11\"/> <edge from=\"C11\" to=\"N1\" id=\"C11_N1\"/>\n",
        "    <edge from=\"N2\" to=\"C12\" id=\"N2_C12\"/> <edge from=\"C12\" to=\"N2\" id=\"C12_N2\"/>\n",
        "    <edge from=\"S1\" to=\"C21\" id=\"S1_C21\"/> <edge from=\"C21\" to=\"S1\" id=\"C21_S1\"/>\n",
        "    <edge from=\"S2\" to=\"C22\" id=\"S2_C22\"/> <edge from=\"C22\" to=\"S2\" id=\"C22_S2\"/>\n",
        "    <edge from=\"C11\" to=\"C12\" id=\"C11_C12\"/> <edge from=\"C12\" to=\"C11\" id=\"C12_C11\"/>\n",
        "    <edge from=\"C21\" to=\"C22\" id=\"C21_C22\"/> <edge from=\"C22\" to=\"C21\" id=\"C22_C21\"/>\n",
        "    <edge from=\"C11\" to=\"C21\" id=\"C11_C21\"/> <edge from=\"C21\" to=\"C11\" id=\"C21_C11\"/>\n",
        "    <edge from=\"C12\" to=\"C22\" id=\"C12_C22\"/> <edge from=\"C22\" to=\"C12\" id=\"C22_C12\"/>\n",
        "</edges>\"\"\")\n",
        "\n",
        "# Conversión a .net.xml\n",
        "!netconvert --node-files=data/2_2_intersections.nod.xml --edge-files=data/2_2_intersections.edg.xml --output-file=data/2_2_intersections.net.xml > /dev/null\n",
        "print(\"✅ Escenario 2x2 creado en 'data/2_2_intersections.net.xml'\")\n",
        "\n",
        "# --- 1.3 CREACIÓN DEL ARCHIVO DE RUTAS ---\n",
        "print(\"🚗 Creando archivo de rutas de tráfico...\")\n",
        "with open(\"data/2_2_intersections.rou.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<routes>\n",
        "    <vType id=\"car\" accel=\"2.6\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" maxSpeed=\"70\"/>\n",
        "    <flow id=\"f_W1_E1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"180\" from=\"W1_C11\" to=\"C12_E1\"/>\n",
        "    <flow id=\"f_W2_E2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"W2_C21\" to=\"C22_E2\"/>\n",
        "    <flow id=\"f_E1_W1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"180\" from=\"E1_C12\" to=\"C11_W1\"/>\n",
        "    <flow id=\"f_E2_W2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"E2_C22\" to=\"C21_W2\"/>\n",
        "    <flow id=\"f_N1_S1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"N1_C11\" to=\"C21_S1\"/>\n",
        "    <flow id=\"f_N2_S2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"N2_C12\" to=\"C22_S2\"/>\n",
        "    <flow id=\"f_S1_N1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"S1_C21\" to=\"C11_N1\"/>\n",
        "    <flow id=\"f_S2_N2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"S2_C22\" to=\"C12_N2\"/>\n",
        "</routes>\"\"\")\n",
        "print(\"✅ Archivo de rutas creado.\")\n",
        "\n",
        "print(\"\\n\\n--- CONFIGURACIÓN INICIAL COMPLETADA ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5aoNNdsccMW",
        "outputId": "7c1d86ba-5f7b-44da-f9e7-e0e65b34ab71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Instalando SUMO y herramientas...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 21.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "✅ SUMO y herramientas de video instalados y configurados.\n",
            "\n",
            "🐍 Instalando bibliotecas de Python...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Bibliotecas de Python (incluyendo traci) instaladas.\n",
            "\n",
            "📂 Creando escenario de red 2x2...\n",
            "/bin/bash: line 1:  4192 Segmentation fault      (core dumped) netconvert --node-files=data/2_2_intersections.nod.xml --edge-files=data/2_2_intersections.edg.xml --output-file=data/2_2_intersections.net.xml > /dev/null\n",
            "✅ Escenario 2x2 creado en 'data/2_2_intersections.net.xml'\n",
            "🚗 Creando archivo de rutas de tráfico...\n",
            "✅ Archivo de rutas creado.\n",
            "\n",
            "\n",
            "--- CONFIGURACIÓN INICIAL COMPLETADA ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 2: CREACIÓN DE ARCHIVOS DE AYUDA (UTILS)\n",
        "# ===================================================================\n",
        "import os\n",
        "os.makedirs('utils', exist_ok=True)\n",
        "print(\"📂 Creando directorio 'utils'...\")\n",
        "\n",
        "with open(\"utils/net_scrape.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "def get_intersections(filepath):\n",
        "    arr = []\n",
        "    tree = ET.parse(filepath)\n",
        "    root = tree.getroot()\n",
        "    for c in root.iter('junction'):\n",
        "        node = c.attrib['id']\n",
        "        if c.attrib['type'] == 'internal' or not 'intersection' in node:\n",
        "            continue\n",
        "        arr.append(node)\n",
        "    return arr\n",
        "\n",
        "def get_intersection_neighborhoods(filepath):\n",
        "    tree = ET.parse(filepath)\n",
        "    root = tree.getroot()\n",
        "    dic = {}\n",
        "    for c in root.iter('junction'):\n",
        "        node = c.attrib['id']\n",
        "        if c.attrib['type'] == 'internal' or 'intersection' not in node:\n",
        "            continue\n",
        "        dic[node] = []\n",
        "        incLanes = c.attrib['incLanes']\n",
        "        incLanes_list = incLanes.split(' ')\n",
        "        for lane in incLanes_list:\n",
        "            lane_split = lane.split('___')[0]\n",
        "            if 'intersection' in lane_split:\n",
        "                dic[node].append(lane_split)\n",
        "    max_neighbors = max([len(v) for v in dic.values()]) if dic else 0\n",
        "    return dic, max_neighbors + 1\n",
        "\"\"\")\n",
        "\n",
        "with open(\"utils/utils.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "\n",
        "class Counter:\n",
        "    def __init__(self):\n",
        "        self.val = mp.Value(\"i\", 0)\n",
        "        self.lock = mp.Lock()\n",
        "    def get(self):\n",
        "        with self.lock: return self.val.value\n",
        "    def increment(self, amt=1):\n",
        "        with self.lock: self.val.value += amt\n",
        "\n",
        "def tensor(x, device):\n",
        "    if isinstance(x, torch.Tensor): return x\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    return torch.tensor(x, device=device, dtype=torch.float32)\n",
        "\n",
        "def random_sample(indices, batch_size):\n",
        "    indices = np.asarray(np.random.permutation(indices))\n",
        "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
        "    for batch in batches: yield batch\n",
        "    r = len(indices) % batch_size\n",
        "    if r: yield indices[-r:]\n",
        "\n",
        "def layer_init_filter(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def ensure_shared_grads(model, shared_model):\n",
        "    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
        "        if shared_param.grad is not None: return\n",
        "        shared_param._grad = param.grad\n",
        "\"\"\")\n",
        "print(\"✅ Archivos de ayuda creados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkxsKk4ycg0S",
        "outputId": "9f4ad2db-3b56-497c-9e90-a5942c2b363a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Creando directorio 'utils'...\n",
            "✅ Archivos de ayuda creados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 3: DEFINICIÓN DEL ENTORNO\n",
        "# ===================================================================\n",
        "import os\n",
        "os.makedirs('environments', exist_ok=True)\n",
        "print(\"📂 Creando directorio 'environments'...\")\n",
        "\n",
        "with open(\"environments/intersections.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from utils.net_scrape import get_intersections, get_intersection_neighborhoods\n",
        "\n",
        "# CONSTANTES GLOBALES\n",
        "PER_AGENT_STATE_SIZE = 4 # Jam_length(2) + Phase(1) + Elapsed(1)\n",
        "GLOBAL_STATE_SIZE = 1 # Sim_step\n",
        "ACTION_SIZE = 2 # NS o EW\n",
        "\n",
        "class SumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600, agent_id=\"default\"):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.agent_id = agent_id\n",
        "        self.label = str(agent_id)\n",
        "\n",
        "        self.intersections = get_intersections(self.net_file)\n",
        "        self.vehicles = dict()\n",
        "        self.traci = None\n",
        "\n",
        "        self._start_simulation()\n",
        "        self.lanes = self.traci.lane.getIDList()\n",
        "        self.edge_ids = self.traci.edge.getIDList()\n",
        "        self.intersection_lanes = {intr: self._get_lanes(intr) for intr in self.intersections}\n",
        "\n",
        "    def _get_lanes(self, intersection_id):\n",
        "        lanes = set()\n",
        "        for edge in self.edge_ids:\n",
        "            if edge.startswith(\":\"): continue\n",
        "            if self.traci.lane.getEdgeID(edge + \"_0\") == edge:\n",
        "                if self.traci.edge.getToJunction(edge) == intersection_id:\n",
        "                    lanes.add(edge + \"_0\")\n",
        "        return list(lanes)\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        sumo_cmd = [\n",
        "            sumo_binary,\n",
        "            '-n', self.net_file,\n",
        "            '-r', self.route_file,\n",
        "            '--time-to-teleport', '-1',\n",
        "            '--no-warnings', 'true'\n",
        "        ]\n",
        "        traci.start(sumo_cmd, label=self.label)\n",
        "        self.traci = traci.getConnection(self.label)\n",
        "\n",
        "    def reset(self):\n",
        "        if self.traci is not None:\n",
        "            self.traci.close()\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        state = [[] for _ in self.intersections]\n",
        "        sim_time = self.traci.simulation.getTime() / self.num_seconds\n",
        "\n",
        "        for i, intr in enumerate(self.intersections):\n",
        "            # Jam length por carril de entrada\n",
        "            jam_len = [self.traci.lane.getLastStepHaltingNumber(lane) for lane in self.intersection_lanes[intr]]\n",
        "            # Phase\n",
        "            phase = self.traci.trafficlight.getPhase(intr)\n",
        "            # Tiempo en la fase\n",
        "            elapsed = self.traci.trafficlight.getPhaseDuration(intr) - self.traci.trafficlight.getNextSwitch(intr) + sim_time*self.num_seconds\n",
        "\n",
        "            # Normalizar y agregar\n",
        "            state[i].extend([val / 10.0 for val in jam_len]) # Normaliza\n",
        "            state[i].append(phase / 3.0) # Normaliza\n",
        "            state[i].append(elapsed / 100.0) # Normaliza\n",
        "            state[i].append(sim_time)\n",
        "\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, intr in enumerate(self.intersections):\n",
        "            current_phase = self.traci.trafficlight.getPhase(intr)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0: # Solo cambia si es verde y la acción es 1\n",
        "                self.traci.trafficlight.setPhase(intr, (current_phase + 1) % 4)\n",
        "\n",
        "        self.traci.simulationStep()\n",
        "\n",
        "        state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0\n",
        "\n",
        "        return state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward = -total_wait_time\n",
        "\n",
        "        rewards = np.array([reward / len(self.intersections)] * len(self.intersections))\n",
        "        return rewards\n",
        "\n",
        "    def close(self):\n",
        "        if self.traci:\n",
        "            self.traci.close()\n",
        "\"\"\")\n",
        "print(\"✅ Entorno multi-agente creado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5scktFb8cpVX",
        "outputId": "478aa3c2-964c-40bd-f75b-edb93b20fbed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Creando directorio 'environments'...\n",
            "✅ Entorno multi-agente creado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 4: MODELO PPO Y WORKER\n",
        "# ===================================================================\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('workers', exist_ok=True)\n",
        "print(\"📂 Creando directorios 'models' y 'workers'...\")\n",
        "\n",
        "# --- Modelo PPO (Actor-Crítico) ---\n",
        "with open(\"models/ppo_model.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from utils.utils import layer_init_filter\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # Red del Actor\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        # Red del Crítico\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.apply(layer_init_filter)\n",
        "\n",
        "    def forward(self, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        return action.detach(), action_logprob.detach()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\"\"\")\n",
        "\n",
        "# --- Worker (Lógica de entrenamiento por agente) ---\n",
        "with open(\"workers/ppo_worker.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "from utils.utils import tensor\n",
        "from environments.intersections import SumoEnv\n",
        "\n",
        "class PPOWorker:\n",
        "    def __init__(self, agent_id, shared_model, optimizer, constants, device):\n",
        "        self.agent_id = agent_id\n",
        "        self.device = device\n",
        "        self.constants = constants\n",
        "        self.gamma = constants['ppo']['discount']\n",
        "        self.eps_clip = constants['ppo']['ppo_ratio_clip']\n",
        "        self.K_epochs = constants['ppo']['optimization_epochs']\n",
        "\n",
        "        self.env = SumoEnv(\n",
        "            net_file='data/2_2_intersections.net.xml',\n",
        "            route_file='data/2_2_intersections.rou.xml',\n",
        "            agent_id=f\"worker_{self.agent_id}\"\n",
        "        )\n",
        "\n",
        "        self.state_dim = self.env._get_state().shape[1]\n",
        "        self.action_dim = 2 # NS o EW\n",
        "\n",
        "        self.policy = shared_model\n",
        "        self.optimizer = optimizer\n",
        "        self.MseLoss = torch.nn.MSELoss()\n",
        "\n",
        "    def train(self, memory_buffer):\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory_buffer['rewards']), reversed(memory_buffer['is_terminals'])):\n",
        "            if is_terminal: discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        old_states = torch.squeeze(torch.stack(memory_buffer['states'], dim=0)).to(self.device).detach()\n",
        "        old_actions = torch.squeeze(torch.stack(memory_buffer['actions'], dim=0)).to(self.device).detach()\n",
        "        old_logprobs = torch.squeeze(torch.stack(memory_buffer['logprobs'], dim=0)).to(self.device).detach()\n",
        "\n",
        "        for _ in range(self.K_epochs):\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\"\"\")\n",
        "print(\"✅ Modelo PPO y Worker creados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlqYETI4dQ-k",
        "outputId": "6c03df09-3570-4a33-a580-0cd4fdcb4259"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Creando directorios 'models' y 'workers'...\n",
            "✅ Modelo PPO y Worker creados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 5: SCRIPT PRINCIPAL DE ENTRENAMIENTO\n",
        "# ===================================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "from models.ppo_model import ActorCritic\n",
        "from workers.ppo_worker import PPOWorker\n",
        "import time\n",
        "\n",
        "# --- HIPERPARÁMETROS ---\n",
        "CONSTANTS = {\n",
        "    \"ppo\": {\n",
        "        \"discount\": 0.99,\n",
        "        \"ppo_ratio_clip\": 0.2,\n",
        "        \"optimization_epochs\": 80,\n",
        "        \"learning_rate\": 0.0003\n",
        "    },\n",
        "    \"max_episodes\": 1000,\n",
        "    \"max_timesteps\": 1500,\n",
        "    \"update_timestep\": 4000\n",
        "}\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_AGENTS = 4 # Para una cuadrícula 2x2\n",
        "STATE_DIM = 6 # Dimensión del estado por agente (ajustar si cambia en el entorno)\n",
        "ACTION_DIM = 2 # 2 acciones posibles (NS, EW)\n",
        "HIDDEN_DIM = 64\n",
        "MODELO_GUARDADO = \"ppo_multi_agent_traffic.pth\"\n",
        "\n",
        "\n",
        "# --- LÓGICA DE ENTRENAMIENTO ---\n",
        "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# Crear un modelo compartido y optimizador\n",
        "shared_model = ActorCritic(STATE_DIM, ACTION_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(shared_model.parameters(), lr=CONSTANTS[\"ppo\"][\"learning_rate\"])\n",
        "\n",
        "# Búfer de memoria para cada agente\n",
        "memory_buffers = [{'states': [], 'actions': [], 'logprobs': [], 'rewards': [], 'is_terminals': []} for _ in range(NUM_AGENTS)]\n",
        "workers = [PPOWorker(i, shared_model, optimizer, CONSTANTS, DEVICE) for i in range(NUM_AGENTS)]\n",
        "\n",
        "print(\"🚦 Inicializando workers y entorno...\")\n",
        "states = [worker.env.reset() for worker in workers]\n",
        "states = np.array(states).reshape(NUM_AGENTS, -1) # Aplanar estados\n",
        "\n",
        "timestep_count = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# Bucle principal de entrenamiento\n",
        "for i_episode in range(1, CONSTANTS[\"max_episodes\"] + 1):\n",
        "    for t in range(CONSTANTS[\"max_timesteps\"]):\n",
        "        timestep_count += 1\n",
        "\n",
        "        # Cada agente elige una acción\n",
        "        actions = []\n",
        "        log_probs = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(NUM_AGENTS):\n",
        "                state_tensor = torch.FloatTensor(states[i]).to(DEVICE)\n",
        "                action, log_prob = shared_model.act(state_tensor)\n",
        "                actions.append(action)\n",
        "                log_probs.append(log_prob)\n",
        "\n",
        "        # Ejecutar acción en cada entorno de worker y recolectar datos\n",
        "        next_states, rewards, dones = [], [], []\n",
        "        for i, worker in enumerate(workers):\n",
        "            # El step del entorno espera una lista de acciones\n",
        "            # Aquí lo simplificamos a que cada worker maneje su entorno\n",
        "            # En una implementación real, se enviaría el vector `actions`\n",
        "            s_prime, r, done = worker.env.step([actions[i].item()])\n",
        "            next_states.append(s_prime)\n",
        "            rewards.append(r)\n",
        "            dones.append(done)\n",
        "\n",
        "            memory_buffers[i]['states'].append(torch.FloatTensor(states[i]))\n",
        "            memory_buffers[i]['actions'].append(actions[i])\n",
        "            memory_buffers[i]['logprobs'].append(log_probs[i])\n",
        "            memory_buffers[i]['rewards'].append(rewards[i][0]) # Tomar el reward escalar\n",
        "            memory_buffers[i]['is_terminals'].append(done)\n",
        "\n",
        "        states = np.array(next_states).reshape(NUM_AGENTS, -1)\n",
        "\n",
        "        # Actualización de la política\n",
        "        if timestep_count % CONSTANTS[\"update_timestep\"] == 0:\n",
        "            print(f\"🧠 Actualizando política en el timestep {timestep_count}...\")\n",
        "            for i in range(NUM_AGENTS):\n",
        "                workers[i].train(memory_buffers[i])\n",
        "                # Limpiar buffer después de actualizar\n",
        "                memory_buffers[i] = {'states': [], 'actions': [], 'logprobs': [], 'rewards': [], 'is_terminals': []}\n",
        "\n",
        "            timestep_count = 0\n",
        "\n",
        "        if any(dones):\n",
        "            break\n",
        "\n",
        "    # Guardar el modelo y mostrar progreso\n",
        "    if i_episode % 50 == 0:\n",
        "        print(f\"Episodio: {i_episode}, Recompensa promedio: {np.mean(rewards):.2f}, Tiempo: {time.time() - start_time:.2f}s\")\n",
        "        torch.save(shared_model.state_dict(), MODELO_GUARDADO)\n",
        "        print(f\"💾 Modelo guardado como '{MODELO_GUARDADO}'\")\n",
        "\n",
        "print(\"\\n\\n--- ENTRENAMIENTO FINALIZADO ---\")\n",
        "for worker in workers:\n",
        "    worker.env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "LN-N32oidVeQ",
        "outputId": "6617f8ba-6931-4d1f-e3b6-46a44d7f248e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/traci/__init__.py:42: UserWarning: Could not import libsumo using /usr/bin/python3, falling back to pure python traci (No module named 'libsumo').\n",
            "  warnings.warn(\"Could not import libsumo using %s, falling back to pure python traci (%s).\" %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Usando dispositivo: cpu\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1666632860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Búfer de memoria para cada agente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmemory_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logprobs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rewards'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_terminals'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPPOWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONSTANTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚦 Inicializando workers y entorno...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-1666632860.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Búfer de memoria para cada agente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmemory_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logprobs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rewards'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_terminals'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPPOWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONSTANTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚦 Inicializando workers y entorno...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/workers/ppo_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_id, shared_model, optimizer, constants, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# NS o EW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "agahWT6ggy_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xqY95DQWgzCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZhGXUQvgzF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 1: INSTALACIÓN\n",
        "# ===================================================================\n",
        "import os\n",
        "print(\"⚙️ Instalando SUMO...\")\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo add-apt-repository -y ppa:sumo/stable > /dev/null\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo apt-get install -y sumo sumo-tools sumo-doc > /dev/null\n",
        "os.environ['SUMO_HOME'] = '/usr/share/sumo'\n",
        "print(\"✅ SUMO instalado.\")\n",
        "print(\"\\n🐍 Instalando bibliotecas de RL...\")\n",
        "!pip install -q torch traci\n",
        "print(\"✅ Bibliotecas de RL instaladas.\")\n",
        "\n",
        "# ===================================================================\n",
        "# CELDA 2: CREACIÓN DEL ESCENARIO 2x2\n",
        "# ===================================================================\n",
        "escenario_dir = 'mi_escenario_grid2x2'\n",
        "os.makedirs(escenario_dir, exist_ok=True)\n",
        "print(f\"📂 Creando escenario en '{escenario_dir}'...\")\n",
        "with open(f\"{escenario_dir}/grid.nod.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<nodes>\n",
        "    <node id=\"C11\" x=\"0\" y=\"100\" type=\"traffic_light\"/> <node id=\"C12\" x=\"100\" y=\"100\" type=\"traffic_light\"/>\n",
        "    <node id=\"C21\" x=\"0\" y=\"0\" type=\"traffic_light\"/>   <node id=\"C22\" x=\"100\" y=\"0\" type=\"traffic_light\"/>\n",
        "    <node id=\"N1\" x=\"0\" y=\"200\"/> <node id=\"N2\" x=\"100\" y=\"200\"/>\n",
        "    <node id=\"S1\" x=\"0\" y=\"-100\"/> <node id=\"S2\" x=\"100\" y=\"-100\"/>\n",
        "    <node id=\"W1\" x=\"-100\" y=\"100\"/> <node id=\"W2\" x=\"-100\" y=\"0\"/>\n",
        "    <node id=\"E1\" x=\"200\" y=\"100\"/> <node id=\"E2\" x=\"200\" y=\"0\"/>\n",
        "</nodes>\"\"\")\n",
        "with open(f\"{escenario_dir}/grid.edg.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<edges>\n",
        "    <edge from=\"W1\" to=\"C11\" id=\"W1_C11\"/> <edge from=\"C11\" to=\"W1\" id=\"C11_W1\"/>\n",
        "    <edge from=\"W2\" to=\"C21\" id=\"W2_C21\"/> <edge from=\"C21\" to=\"W2\" id=\"C21_W2\"/>\n",
        "    <edge from=\"E1\" to=\"C12\" id=\"E1_C12\"/> <edge from=\"C12\" to=\"E1\" id=\"C12_E1\"/>\n",
        "    <edge from=\"E2\" to=\"C22\" id=\"E2_C22\"/> <edge from=\"C22\" to=\"E2\" id=\"C22_E2\"/>\n",
        "    <edge from=\"N1\" to=\"C11\" id=\"N1_C11\"/> <edge from=\"C11\" to=\"N1\" id=\"C11_N1\"/>\n",
        "    <edge from=\"N2\" to=\"C12\" id=\"N2_C12\"/> <edge from=\"C12\" to=\"N2\" id=\"C12_N2\"/>\n",
        "    <edge from=\"S1\" to=\"C21\" id=\"S1_C21\"/> <edge from=\"C21\" to=\"S1\" id=\"C21_S1\"/>\n",
        "    <edge from=\"S2\" to=\"C22\" id=\"S2_C22\"/> <edge from=\"C22\" to=\"S2\" id=\"C22_S2\"/>\n",
        "    <edge from=\"C11\" to=\"C12\" id=\"C11_C12\"/> <edge from=\"C12\" to=\"C11\" id=\"C12_C11\"/>\n",
        "    <edge from=\"C21\" to=\"C22\" id=\"C21_C22\"/> <edge from=\"C22\" to=\"C21\" id=\"C22_C21\"/>\n",
        "    <edge from=\"C11\" to=\"C21\" id=\"C11_C21\"/> <edge from=\"C21\" to=\"C11\" id=\"C21_C11\"/>\n",
        "    <edge from=\"C12\" to=\"C22\" id=\"C12_C22\"/> <edge from=\"C22\" to=\"C12\" id=\"C22_C12\"/>\n",
        "</edges>\"\"\")\n",
        "!netconvert --node-files={escenario_dir}/grid.nod.xml --edge-files={escenario_dir}/grid.edg.xml --output-file={escenario_dir}/grid.net.xml > /dev/null\n",
        "with open(f\"{escenario_dir}/grid.rou.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<routes>\n",
        "    <vType id=\"car\" accel=\"2.6\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" maxSpeed=\"70\"/>\n",
        "    <flow id=\"f_we\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"W1_C11\" to=\"C12_E1\"/>\n",
        "    <flow id=\"f_ew\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"E2_C22\" to=\"C21_W2\"/>\n",
        "    <flow id=\"f_ns\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"N1_C11\" to=\"C21_S1\"/>\n",
        "    <flow id=\"f_sn\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"S2_C22\" to=\"C12_N2\"/>\n",
        "</routes>\"\"\")\n",
        "print(\"✅ Escenario 2x2 creado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J2ww-rkgzR5",
        "outputId": "3432fc5c-2555-4c6a-e45a-071624e19287"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Instalando SUMO...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 21.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "✅ SUMO instalado.\n",
            "\n",
            "🐍 Instalando bibliotecas de RL...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Bibliotecas de RL instaladas.\n",
            "📂 Creando escenario en 'mi_escenario_grid2x2'...\n",
            "/bin/bash: line 1:  3517 Segmentation fault      (core dumped) netconvert --node-files=mi_escenario_grid2x2/grid.nod.xml --edge-files=mi_escenario_grid2x2/grid.edg.xml --output-file=mi_escenario_grid2x2/grid.net.xml > /dev/null\n",
            "✅ Escenario 2x2 creado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 3: ENTORNO, MODELO Y ENTRENAMIENTO (VERSIÓN SIMPLIFICADA)\n",
        "# ===================================================================\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import time\n",
        "\n",
        "# --- 1. CLASE DE ENTORNO (SIMPLIFICADA) ---\n",
        "class MultiAgentSumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        self.ts_ids = []\n",
        "        self.lanes = []\n",
        "        self._start_simulation()\n",
        "        self.ts_ids = list(self.traci.trafficlight.getIDList())\n",
        "        self.lanes = list(self.traci.lane.getIDList())\n",
        "        self.state_dim = 5 # phase, avg_wait_time, avg_speed, density, queue\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        try:\n",
        "            traci.close()\n",
        "        except:\n",
        "            pass\n",
        "        traci.start([self.sumo_binary, \"-n\", self.net_file, \"-r\", self.route_file, \"--time-to-teleport\", \"-1\", \"--no-warnings\", \"true\"])\n",
        "        self.traci = traci\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        states = []\n",
        "        for ts_id in self.ts_ids:\n",
        "            phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            incoming_lanes = set()\n",
        "            for link in self.traci.trafficlight.getControlledLinks(ts_id):\n",
        "                incoming_lanes.add(link[0][0])\n",
        "\n",
        "            if not incoming_lanes:\n",
        "                states.append(np.zeros(self.state_dim))\n",
        "                continue\n",
        "\n",
        "            avg_wait_time = np.mean([self.traci.lane.getWaitingTime(lane) for lane in incoming_lanes])\n",
        "            avg_speed = np.mean([self.traci.lane.getLastStepMeanSpeed(lane) for lane in incoming_lanes])\n",
        "            density = np.mean([self.traci.lane.getLastStepOccupancy(lane) for lane in incoming_lanes])\n",
        "            queue = np.mean([self.traci.lane.getLastStepHaltingNumber(lane) for lane in incoming_lanes])\n",
        "\n",
        "            state = [phase/3.0, avg_wait_time/100.0, avg_speed/20.0, density, queue/20.0]\n",
        "            states.append(np.array(state, dtype=np.float32))\n",
        "        return np.array(states)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, ts_id in enumerate(self.ts_ids):\n",
        "            current_phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0:\n",
        "                self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n",
        "\n",
        "        self.traci.simulationStep()\n",
        "\n",
        "        next_state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0 or self.traci.simulation.getTime() >= self.num_seconds\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward_per_agent = -total_wait_time / len(self.ts_ids)\n",
        "        return np.array([reward_per_agent] * len(self.ts_ids))\n",
        "\n",
        "    def close(self):\n",
        "        traci.close()\n",
        "\n",
        "# --- 2. CLASE DEL MODELO ACTOR-CRÍTICO ---\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.detach(), dist.log_prob(action).detach()\n",
        "\n",
        "# --- 3. LÓGICA DE ENTRENAMIENTO ---\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# --- Hiperparámetros ---\n",
        "TOTAL_TIMESTEPS = 20000\n",
        "TIMESTEPS_POR_ACTUALIZACION = 2048\n",
        "LR = 0.002\n",
        "GAMMA = 0.99\n",
        "\n",
        "# --- Inicialización ---\n",
        "env = MultiAgentSumoEnv(net_file='mi_escenario_grid2x2/grid.net.xml', route_file='mi_escenario_grid2x2/grid.rou.xml')\n",
        "state_dim = env.state_dim\n",
        "action_dim = 2 # 0: quedarse, 1: cambiar\n",
        "num_agents = len(env.ts_ids)\n",
        "\n",
        "policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=LR)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# --- Bucle de entrenamiento ---\n",
        "timestep_count = 0\n",
        "while timestep_count < TOTAL_TIMESTEPS:\n",
        "    states, log_probs, rewards, dones = [], [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    # Recolectar un batch de experiencia\n",
        "    for t in range(TIMESTEPS_POR_ACTUALIZACION):\n",
        "        timestep_count += 1\n",
        "\n",
        "        # Actuar\n",
        "        state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "        actions, log_prob = policy.act(state_tensor)\n",
        "        next_state, reward, done = env.step(actions.cpu().numpy())\n",
        "\n",
        "        # Guardar experiencia\n",
        "        states.append(state_tensor)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(torch.FloatTensor(reward).to(DEVICE))\n",
        "        dones.append(torch.FloatTensor([done] * num_agents).to(DEVICE))\n",
        "\n",
        "        state = next_state\n",
        "        if done: break\n",
        "\n",
        "    print(f\"Timestep: {timestep_count}/{TOTAL_TIMESTEPS}, Recompensa del último paso: {np.mean(reward):.3f}\")\n",
        "\n",
        "    # Calcular recompensas descontadas (rewards-to-go)\n",
        "    discounted_rewards = []\n",
        "    cumulative_reward = torch.zeros(num_agents).to(DEVICE)\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        cumulative_reward = reward + GAMMA * cumulative_reward * (1 - done)\n",
        "        discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "    # Actualizar la política\n",
        "    states = torch.stack(states).to(DEVICE).view(-1, state_dim)\n",
        "    old_log_probs = torch.stack(log_probs).to(DEVICE).view(-1)\n",
        "    discounted_rewards = torch.stack(discounted_rewards).to(DEVICE).view(-1)\n",
        "\n",
        "    # Evaluación del valor del estado\n",
        "    state_values = policy.critic(states).squeeze()\n",
        "\n",
        "    # Calcular la pérdida y optimizar\n",
        "    advantages = discounted_rewards - state_values.detach()\n",
        "    actor_loss = -(advantages * old_log_probs).mean()\n",
        "    critic_loss = mse_loss(state_values, discounted_rewards)\n",
        "\n",
        "    loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# --- Limpieza y guardado final ---\n",
        "torch.save(policy.state_dict(), 'ppo_multi_agente_final_20k.pth')\n",
        "env.close()\n",
        "print(\"\\n--- ✅ ENTRENAMIENTO FINALIZADO. Modelo guardado como 'ppo_multi_agente_final.pth' ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjJiqD69g71w",
        "outputId": "be25beda-92a7-4186-b39b-1faf37471b70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Usando dispositivo: cuda\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1547248210.py:64: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
            "  self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestep: 2048/20000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 4096/20000, Recompensa del último paso: -0.500\n",
            " Retrying in 1 seconds\n",
            "Timestep: 6144/20000, Recompensa del último paso: -1.750\n",
            " Retrying in 1 seconds\n",
            "Timestep: 8192/20000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 10240/20000, Recompensa del último paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 12288/20000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 14336/20000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 16384/20000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 18432/20000, Recompensa del último paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 20480/20000, Recompensa del último paso: 0.000\n",
            "\n",
            "--- ✅ ENTRENAMIENTO FINALIZADO. Modelo guardado como 'ppo_multi_agente_final.pth' ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 4: MÓDULO DE EVALUACIÓN Y COMPARACIÓN\n",
        "# ===================================================================\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "print(\"--- 📊 Iniciando Evaluación Comparativa ---\")\n",
        "\n",
        "# --- PARÁMETROS ---\n",
        "ESCENARIO_DIR = 'mi_escenario_grid2x2'\n",
        "MODELO_A_CARGAR = 'ppo_multi_agente_final_20k.pth'\n",
        "ARCHIVO_RESULTADOS_CSV = f\"20k_resultados_comparativos_{MODELO_A_CARGAR.replace('.pth', '')}.csv\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# --- CLASES NECESARIAS (Copiadas de la celda de entrenamiento) ---\n",
        "# Necesitamos redefinir las clases aquí para que la celda sea autónoma.\n",
        "class MultiAgentSumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600, additional_sumo_cmd=None):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.additional_sumo_cmd = additional_sumo_cmd or []\n",
        "        self.sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        self._start_simulation()\n",
        "        self.ts_ids = list(self.traci.trafficlight.getIDList())\n",
        "        self.lanes = list(self.traci.lane.getIDList())\n",
        "        self.state_dim = 5\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        try: traci.close()\n",
        "        except: pass\n",
        "        sumo_cmd = [self.sumo_binary, \"-n\", self.net_file, \"-r\", self.route_file, \"--time-to-teleport\", \"-1\", \"--no-warnings\", \"true\"]\n",
        "        sumo_cmd.extend(self.additional_sumo_cmd)\n",
        "        traci.start(sumo_cmd)\n",
        "        self.traci = traci\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        states = []\n",
        "        for ts_id in self.ts_ids:\n",
        "            phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            incoming_lanes = {link[0][0] for link in self.traci.trafficlight.getControlledLinks(ts_id)}\n",
        "            if not incoming_lanes:\n",
        "                states.append(np.zeros(self.state_dim))\n",
        "                continue\n",
        "            avg_wait_time = np.mean([self.traci.lane.getWaitingTime(lane) for lane in incoming_lanes])\n",
        "            avg_speed = np.mean([self.traci.lane.getLastStepMeanSpeed(lane) for lane in incoming_lanes])\n",
        "            density = np.mean([self.traci.lane.getLastStepOccupancy(lane) for lane in incoming_lanes])\n",
        "            queue = np.mean([self.traci.lane.getLastStepHaltingNumber(lane) for lane in incoming_lanes])\n",
        "            state = [phase/3.0, avg_wait_time/100.0, avg_speed/20.0, density, queue/20.0]\n",
        "            states.append(np.array(state, dtype=np.float32))\n",
        "        return np.array(states)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, ts_id in enumerate(self.ts_ids):\n",
        "            current_phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0:\n",
        "                self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n",
        "        self.traci.simulationStep()\n",
        "        next_state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0 or self.traci.simulation.getTime() >= self.num_seconds\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward_per_agent = -total_wait_time / len(self.ts_ids)\n",
        "        return np.array([reward_per_agent] * len(self.ts_ids))\n",
        "\n",
        "    def close(self):\n",
        "        traci.close()\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1))\n",
        "        self.critic = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.detach(), dist.log_prob(action).detach()\n",
        "\n",
        "# --- 1. Simular y Grabar el Caso Base (Tiempos Fijos) ---\n",
        "print(\"\\n1/3: 🚦 Ejecutando simulación del Caso Base (tiempos fijos)...\")\n",
        "sumo_cmd_base = [\n",
        "    sumolib.checkBinary('sumo'),\n",
        "    \"-n\", f\"{ESCENARIO_DIR}/grid.net.xml\",\n",
        "    \"-r\", f\"{ESCENARIO_DIR}/grid.rou.xml\",\n",
        "    \"--tripinfo-output\", f\"{ESCENARIO_DIR}/tripinfo_base.xml\",\n",
        "    \"--time-to-teleport\", \"-1\",\n",
        "    \"--no-warnings\", \"true\"\n",
        "]\n",
        "traci.start(sumo_cmd_base)\n",
        "while traci.simulation.getMinExpectedNumber() > 0:\n",
        "    traci.simulationStep()\n",
        "traci.close()\n",
        "\n",
        "# --- 2. Simular y Grabar el Agente RL Entrenado ---\n",
        "print(\"2/3: 🤖 Ejecutando simulación del Agente RL...\")\n",
        "# Cargar el modelo entrenado\n",
        "env_eval = MultiAgentSumoEnv(\n",
        "    net_file=f'{ESCENARIO_DIR}/grid.net.xml',\n",
        "    route_file=f'{ESCENARIO_DIR}/grid.rou.xml',\n",
        "    additional_sumo_cmd=[\"--tripinfo-output\", f\"{ESCENARIO_DIR}/tripinfo_entrenado.xml\"]\n",
        ")\n",
        "state_dim = env_eval.state_dim\n",
        "action_dim = 2\n",
        "policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "policy.load_state_dict(torch.load(MODELO_A_CARGAR, map_location=DEVICE))\n",
        "policy.eval() # Poner en modo de evaluación\n",
        "\n",
        "state = env_eval.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        actions, _ = policy.act(state_tensor)\n",
        "    state, _, done = env_eval.step(actions.cpu().numpy())\n",
        "env_eval.close()\n",
        "print(\"✅ Simulaciones completadas.\")\n",
        "\n",
        "# --- 3. Parsear y Analizar los Resultados ---\n",
        "print(\"3/3: 📈 Analizando resultados...\")\n",
        "def parse_tripinfo(xml_file):\n",
        "    try:\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        trips = [{'duration': float(t.get('duration')), 'timeLoss': float(t.get('timeLoss'))} for t in root.findall('tripinfo')]\n",
        "        return pd.DataFrame(trips)\n",
        "    except (FileNotFoundError, ET.ParseError):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "df_base = parse_tripinfo(f'{ESCENARIO_DIR}/tripinfo_base.xml')\n",
        "df_entrenado = parse_tripinfo(f'{ESCENARIO_DIR}/tripinfo_entrenado.xml')\n",
        "\n",
        "if not df_base.empty and not df_entrenado.empty:\n",
        "    avg_base = df_base.mean()\n",
        "    avg_entrenado = df_entrenado.mean()\n",
        "    mejora = ((avg_base - avg_entrenado) / avg_base) * 100\n",
        "    df_resultados = pd.DataFrame({\"Caso Base (Fijo)\": avg_base, \"Agente RL\": avg_entrenado, \"Mejora (%)\": mejora})\n",
        "    df_resultados.index = [\"Duración Media del Viaje (s)\", \"Tiempo Medio Perdido (s)\"]\n",
        "\n",
        "    print(\"\\n\\n--- ANÁLISIS DE RENDIMIENTO ---\")\n",
        "    print(df_resultados.to_string(formatters={'Mejora (%)': '{:,.2f}%'.format}))\n",
        "\n",
        "    df_resultados.to_csv(ARCHIVO_RESULTADOS_CSV)\n",
        "    print(f\"\\n\\n--- ✅ Proceso de evaluación completado. Resultados guardados en '{ARCHIVO_RESULTADOS_CSV}' ---\")\n",
        "else:\n",
        "    print(\"\\n❌ No se pudieron generar las métricas. Asegúrate de que ambas simulaciones se ejecutaron correctamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy_6emG5hAYg",
        "outputId": "21da1a59-197c-4d98-8cfd-8eae0a25364d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 📊 Iniciando Evaluación Comparativa ---\n",
            "\n",
            "1/3: 🚦 Ejecutando simulación del Caso Base (tiempos fijos)...\n",
            " Retrying in 1 seconds\n",
            "2/3: 🤖 Ejecutando simulación del Agente RL...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2701626409.py:70: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
            "  self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Simulaciones completadas.\n",
            "3/3: 📈 Analizando resultados...\n",
            "\n",
            "\n",
            "--- ANÁLISIS DE RENDIMIENTO ---\n",
            "                              Caso Base (Fijo)  Agente RL Mejora (%)\n",
            "Duración Media del Viaje (s)         44.712857  34.317986     23.25%\n",
            "Tiempo Medio Perdido (s)             22.999129  12.589237     45.26%\n",
            "\n",
            "\n",
            "--- ✅ Proceso de evaluación completado. Resultados guardados en '20k_resultados_comparativos_ppo_multi_agente_final_20k.csv' ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 3: ENTRENAMIENTO CON GRÁFICA DE PROGRESO (CORREGIDA)\n",
        "# ===================================================================\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import time\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. CLASE DE ENTORNO (SIN CAMBIOS) ---\n",
        "class MultiAgentSumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600, additional_sumo_cmd=None):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.additional_sumo_cmd = additional_sumo_cmd or []\n",
        "        self.sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        self._start_simulation()\n",
        "        self.ts_ids = list(self.traci.trafficlight.getIDList())\n",
        "        self.lanes = list(self.traci.lane.getIDList())\n",
        "        self.state_dim = 5\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        try: traci.close()\n",
        "        except: pass\n",
        "        sumo_cmd = [self.sumo_binary, \"-n\", self.net_file, \"-r\", self.route_file, \"--time-to-teleport\", \"-1\", \"--no-warnings\", \"true\"]\n",
        "        sumo_cmd.extend(self.additional_sumo_cmd)\n",
        "        traci.start(sumo_cmd)\n",
        "        self.traci = traci\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        states = []\n",
        "        for ts_id in self.ts_ids:\n",
        "            phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            incoming_lanes = {link[0][0] for link in self.traci.trafficlight.getControlledLinks(ts_id)}\n",
        "            if not incoming_lanes:\n",
        "                states.append(np.zeros(self.state_dim))\n",
        "                continue\n",
        "            avg_wait_time = np.mean([self.traci.lane.getWaitingTime(lane) for lane in incoming_lanes])\n",
        "            avg_speed = np.mean([self.traci.lane.getLastStepMeanSpeed(lane) for lane in incoming_lanes])\n",
        "            density = np.mean([self.traci.lane.getLastStepOccupancy(lane) for lane in incoming_lanes])\n",
        "            queue = np.mean([self.traci.lane.getLastStepHaltingNumber(lane) for lane in incoming_lanes])\n",
        "            state = [phase/3.0, avg_wait_time/100.0, avg_speed/20.0, density, queue/20.0]\n",
        "            states.append(np.array(state, dtype=np.float32))\n",
        "        return np.array(states)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, ts_id in enumerate(self.ts_ids):\n",
        "            current_phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0:\n",
        "                self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n",
        "        self.traci.simulationStep()\n",
        "        next_state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0 or self.traci.simulation.getTime() >= self.num_seconds\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward_per_agent = -total_wait_time / len(self.ts_ids) if len(self.ts_ids) > 0 else 0\n",
        "        return np.array([reward_per_agent] * len(self.ts_ids))\n",
        "\n",
        "    def close(self):\n",
        "        traci.close()\n",
        "\n",
        "# --- 2. CLASE DEL MODELO ACTOR-CRÍTICO (SIN CAMBIOS) ---\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1))\n",
        "        self.critic = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.detach(), dist.log_prob(action).detach()\n",
        "\n",
        "# --- NUEVO: Función de evaluación periódica ---\n",
        "def evaluate_policy(policy, net_file, route_file, eval_seconds=1800, device='cpu'):\n",
        "    print(f\"--- 📈 Evaluando rendimiento...\")\n",
        "    eval_env = MultiAgentSumoEnv(\n",
        "        net_file=net_file,\n",
        "        route_file=route_file,\n",
        "        num_seconds=eval_seconds,\n",
        "        additional_sumo_cmd=[\"--tripinfo-output\", \"tripinfo_eval.xml\"]\n",
        "    )\n",
        "    state = eval_env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state).to(device)\n",
        "        with torch.no_grad():\n",
        "            actions, _ = policy.act(state_tensor)\n",
        "        state, _, done = eval_env.step(actions.cpu().numpy())\n",
        "    eval_env.close()\n",
        "\n",
        "    try:\n",
        "        tree = ET.parse(\"tripinfo_eval.xml\")\n",
        "        root = tree.getroot()\n",
        "        times = [float(t.get('timeLoss')) for t in root.findall('tripinfo')]\n",
        "        return np.mean(times) if times else 0\n",
        "    except (FileNotFoundError, ET.ParseError):\n",
        "        return None\n",
        "\n",
        "# --- 3. LÓGICA DE ENTRENAMIENTO (CON CORRECCIONES) ---\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# --- Hiperparámetros ---\n",
        "TOTAL_TIMESTEPS = 25000\n",
        "TIMESTEPS_POR_ACTUALIZACION = 2048\n",
        "EVAL_INTERVAL = 5000\n",
        "LR = 0.002\n",
        "GAMMA = 0.99\n",
        "\n",
        "# --- Inicialización ---\n",
        "env = MultiAgentSumoEnv(net_file='mi_escenario_grid2x2/grid.net.xml', route_file='mi_escenario_grid2x2/grid.rou.xml')\n",
        "state_dim = env.state_dim\n",
        "action_dim = 2\n",
        "num_agents = len(env.ts_ids)\n",
        "policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=LR)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# --- Listas para guardar el historial para la gráfica ---\n",
        "history_timesteps = []\n",
        "history_waiting_time = []\n",
        "last_eval_timestep = 0\n",
        "\n",
        "# --- Bucle de entrenamiento ---\n",
        "timestep_count = 0\n",
        "while timestep_count < TOTAL_TIMESTEPS:\n",
        "    states, log_probs, rewards, dones = [], [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(TIMESTEPS_POR_ACTUALIZACION):\n",
        "        timestep_count += 1\n",
        "        state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "        actions, log_prob = policy.act(state_tensor)\n",
        "        next_state, reward, done = env.step(actions.cpu().numpy())\n",
        "        states.append(state_tensor)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(torch.FloatTensor(reward).to(DEVICE))\n",
        "        dones.append(torch.FloatTensor([done] * num_agents).to(DEVICE))\n",
        "        state = next_state\n",
        "        if done: break\n",
        "\n",
        "    print(f\"Timestep: {timestep_count}/{TOTAL_TIMESTEPS}, Recompensa del último paso: {np.mean(reward):.3f}\")\n",
        "\n",
        "    # --- Bloque de aprendizaje (CON LÍNEAS CORREGIDAS) ---\n",
        "    discounted_rewards = []\n",
        "    cumulative_reward = torch.zeros(num_agents).to(DEVICE)\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        cumulative_reward = reward + GAMMA * cumulative_reward * (1 - done)\n",
        "        discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "    # --- CORRECCIÓN AQUÍ: Se reemplazó `-_1` por `-1` ---\n",
        "    states = torch.stack(states).to(DEVICE).view(-1, state_dim)\n",
        "    old_log_probs = torch.stack(log_probs).to(DEVICE).view(-1)\n",
        "    discounted_rewards = torch.stack(discounted_rewards).to(DEVICE).view(-1)\n",
        "\n",
        "    state_values = policy.critic(states).squeeze()\n",
        "    advantages = discounted_rewards - state_values.detach()\n",
        "    actor_loss = -(advantages * old_log_probs).mean()\n",
        "    critic_loss = mse_loss(state_values, discounted_rewards)\n",
        "    loss = actor_loss + 0.5 * critic_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- Bloque de evaluación periódica ---\n",
        "    if timestep_count - last_eval_timestep >= EVAL_INTERVAL:\n",
        "        avg_wait_time = evaluate_policy(policy, 'mi_escenario_grid2x2/grid.net.xml', 'mi_escenario_grid2x2/grid.rou.xml', device=DEVICE)\n",
        "        if avg_wait_time is not None:\n",
        "            history_timesteps.append(timestep_count)\n",
        "            history_waiting_time.append(avg_wait_time)\n",
        "            print(f\"--- Resultado Evaluación: {avg_wait_time:.2f}s de tiempo de espera promedio ---\\n\")\n",
        "        last_eval_timestep = timestep_count\n",
        "\n",
        "# --- Limpieza y guardado final ---\n",
        "torch.save(policy.state_dict(), 'ppo_multi_agente_final.pth')\n",
        "env.close()\n",
        "print(\"\\n--- ✅ ENTRENAMIENTO FINALIZADO ---\")\n",
        "\n",
        "# --- Graficar el progreso del aprendizaje ---\n",
        "if history_timesteps:\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.plot(history_timesteps, history_waiting_time, marker='o', linestyle='-')\n",
        "    plt.title('Progreso del Entrenamiento: Tiempo de Espera vs. Timesteps', fontsize=16)\n",
        "    plt.xlabel('Timesteps de Entrenamiento', fontsize=12)\n",
        "    plt.ylabel('Tiempo de Espera Promedio (s)', fontsize=12)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcAJKoAg60Wo",
        "outputId": "c532c0f2-9584-4b8e-8ce7-21d433bf54c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Usando dispositivo: cuda\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2956219106.py:61: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
            "  self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestep: 2048/25000, Recompensa del último paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 4096/25000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 6144/25000, Recompensa del último paso: -0.250\n",
            "--- 📈 Evaluando rendimiento...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n",
            "--- Resultado Evaluación: 13.57s de tiempo de espera promedio ---\n",
            "\n",
            " Retrying in 1 seconds\n",
            "Timestep: 8192/25000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 10240/25000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 12288/25000, Recompensa del último paso: -0.250\n",
            "--- 📈 Evaluando rendimiento...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n",
            "--- Resultado Evaluación: 13.38s de tiempo de espera promedio ---\n",
            "\n",
            " Retrying in 1 seconds\n",
            "Timestep: 14336/25000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 16384/25000, Recompensa del último paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 18432/25000, Recompensa del último paso: 0.000\n",
            "--- 📈 Evaluando rendimiento...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n",
            "--- Resultado Evaluación: 13.28s de tiempo de espera promedio ---\n",
            "\n",
            " Retrying in 1 seconds\n",
            "Timestep: 20480/25000, Recompensa del último paso: 0.000\n",
            " Retrying in 1 seconds\n"
          ]
        }
      ]
    }
  ]
}