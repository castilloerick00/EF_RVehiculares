{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 1 (CORREGIDA): INSTALACIÃ“N Y CREACIÃ“N DE ESCENARIOS\n",
        "# ===================================================================\n",
        "import os\n",
        "\n",
        "# --- 1.1 INSTALACIÃ“N DE DEPENDENCIAS ---\n",
        "print(\"âš™ï¸ Instalando SUMO y herramientas...\")\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo add-apt-repository -y ppa:sumo/stable > /dev/null\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo apt-get install -y sumo sumo-tools sumo-doc ffmpeg > /dev/null\n",
        "os.environ['SUMO_HOME'] = '/usr/share/sumo'\n",
        "os.environ['LIBSUMO_AS_TRACI'] = '1'\n",
        "print(\"âœ… SUMO y herramientas de video instalados y configurados.\")\n",
        "\n",
        "# --- CORRECCIÃ“N AQUÃ ---\n",
        "print(\"\\nğŸ Instalando bibliotecas de Python...\")\n",
        "!pip install -q torch pandas numpy traci\n",
        "print(\"âœ… Bibliotecas de Python (incluyendo traci) instaladas.\")\n",
        "\n",
        "\n",
        "# --- 1.2 CREACIÃ“N DEL ESCENARIO GRID 2x2 ---\n",
        "print(\"\\nğŸ“‚ Creando escenario de red 2x2...\")\n",
        "os.makedirs('data', exist_ok=True) # Creamos la carpeta 'data' donde vivirÃ¡ el mapa\n",
        "\n",
        "# Nodos\n",
        "with open(\"data/2_2_intersections.nod.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<nodes>\n",
        "    <node id=\"C11\" x=\"0\" y=\"100\" type=\"traffic_light\"/> <node id=\"C12\" x=\"100\" y=\"100\" type=\"traffic_light\"/>\n",
        "    <node id=\"C21\" x=\"0\" y=\"0\" type=\"traffic_light\"/>   <node id=\"C22\" x=\"100\" y=\"0\" type=\"traffic_light\"/>\n",
        "    <node id=\"N1\" x=\"0\" y=\"200\"/> <node id=\"N2\" x=\"100\" y=\"200\"/>\n",
        "    <node id=\"S1\" x=\"0\" y=\"-100\"/> <node id=\"S2\" x=\"100\" y=\"-100\"/>\n",
        "    <node id=\"W1\" x=\"-100\" y=\"100\"/> <node id=\"W2\" x=\"-100\" y=\"0\"/>\n",
        "    <node id=\"E1\" x=\"200\" y=\"100\"/> <node id=\"E2\" x=\"200\" y=\"0\"/>\n",
        "</nodes>\"\"\")\n",
        "\n",
        "# Aristas\n",
        "with open(\"data/2_2_intersections.edg.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<edges>\n",
        "    <edge from=\"W1\" to=\"C11\" id=\"W1_C11\"/> <edge from=\"C11\" to=\"W1\" id=\"C11_W1\"/>\n",
        "    <edge from=\"W2\" to=\"C21\" id=\"W2_C21\"/> <edge from=\"C21\" to=\"W2\" id=\"C21_W2\"/>\n",
        "    <edge from=\"E1\" to=\"C12\" id=\"E1_C12\"/> <edge from=\"C12\" to=\"E1\" id=\"C12_E1\"/>\n",
        "    <edge from=\"E2\" to=\"C22\" id=\"E2_C22\"/> <edge from=\"C22\" to=\"E2\" id=\"C22_E2\"/>\n",
        "    <edge from=\"N1\" to=\"C11\" id=\"N1_C11\"/> <edge from=\"C11\" to=\"N1\" id=\"C11_N1\"/>\n",
        "    <edge from=\"N2\" to=\"C12\" id=\"N2_C12\"/> <edge from=\"C12\" to=\"N2\" id=\"C12_N2\"/>\n",
        "    <edge from=\"S1\" to=\"C21\" id=\"S1_C21\"/> <edge from=\"C21\" to=\"S1\" id=\"C21_S1\"/>\n",
        "    <edge from=\"S2\" to=\"C22\" id=\"S2_C22\"/> <edge from=\"C22\" to=\"S2\" id=\"C22_S2\"/>\n",
        "    <edge from=\"C11\" to=\"C12\" id=\"C11_C12\"/> <edge from=\"C12\" to=\"C11\" id=\"C12_C11\"/>\n",
        "    <edge from=\"C21\" to=\"C22\" id=\"C21_C22\"/> <edge from=\"C22\" to=\"C21\" id=\"C22_C21\"/>\n",
        "    <edge from=\"C11\" to=\"C21\" id=\"C11_C21\"/> <edge from=\"C21\" to=\"C11\" id=\"C21_C11\"/>\n",
        "    <edge from=\"C12\" to=\"C22\" id=\"C12_C22\"/> <edge from=\"C22\" to=\"C12\" id=\"C22_C12\"/>\n",
        "</edges>\"\"\")\n",
        "\n",
        "# ConversiÃ³n a .net.xml\n",
        "!netconvert --node-files=data/2_2_intersections.nod.xml --edge-files=data/2_2_intersections.edg.xml --output-file=data/2_2_intersections.net.xml > /dev/null\n",
        "print(\"âœ… Escenario 2x2 creado en 'data/2_2_intersections.net.xml'\")\n",
        "\n",
        "# --- 1.3 CREACIÃ“N DEL ARCHIVO DE RUTAS ---\n",
        "print(\"ğŸš— Creando archivo de rutas de trÃ¡fico...\")\n",
        "with open(\"data/2_2_intersections.rou.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<routes>\n",
        "    <vType id=\"car\" accel=\"2.6\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" maxSpeed=\"70\"/>\n",
        "    <flow id=\"f_W1_E1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"180\" from=\"W1_C11\" to=\"C12_E1\"/>\n",
        "    <flow id=\"f_W2_E2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"W2_C21\" to=\"C22_E2\"/>\n",
        "    <flow id=\"f_E1_W1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"180\" from=\"E1_C12\" to=\"C11_W1\"/>\n",
        "    <flow id=\"f_E2_W2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"E2_C22\" to=\"C21_W2\"/>\n",
        "    <flow id=\"f_N1_S1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"N1_C11\" to=\"C21_S1\"/>\n",
        "    <flow id=\"f_N2_S2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"N2_C12\" to=\"C22_S2\"/>\n",
        "    <flow id=\"f_S1_N1\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"S1_C21\" to=\"C11_N1\"/>\n",
        "    <flow id=\"f_S2_N2\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"S2_C22\" to=\"C12_N2\"/>\n",
        "</routes>\"\"\")\n",
        "print(\"âœ… Archivo de rutas creado.\")\n",
        "\n",
        "print(\"\\n\\n--- CONFIGURACIÃ“N INICIAL COMPLETADA ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5aoNNdsccMW",
        "outputId": "7c1d86ba-5f7b-44da-f9e7-e0e65b34ab71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Instalando SUMO y herramientas...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 21.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "âœ… SUMO y herramientas de video instalados y configurados.\n",
            "\n",
            "ğŸ Instalando bibliotecas de Python...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Bibliotecas de Python (incluyendo traci) instaladas.\n",
            "\n",
            "ğŸ“‚ Creando escenario de red 2x2...\n",
            "/bin/bash: line 1:  4192 Segmentation fault      (core dumped) netconvert --node-files=data/2_2_intersections.nod.xml --edge-files=data/2_2_intersections.edg.xml --output-file=data/2_2_intersections.net.xml > /dev/null\n",
            "âœ… Escenario 2x2 creado en 'data/2_2_intersections.net.xml'\n",
            "ğŸš— Creando archivo de rutas de trÃ¡fico...\n",
            "âœ… Archivo de rutas creado.\n",
            "\n",
            "\n",
            "--- CONFIGURACIÃ“N INICIAL COMPLETADA ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 2: CREACIÃ“N DE ARCHIVOS DE AYUDA (UTILS)\n",
        "# ===================================================================\n",
        "import os\n",
        "os.makedirs('utils', exist_ok=True)\n",
        "print(\"ğŸ“‚ Creando directorio 'utils'...\")\n",
        "\n",
        "with open(\"utils/net_scrape.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "def get_intersections(filepath):\n",
        "    arr = []\n",
        "    tree = ET.parse(filepath)\n",
        "    root = tree.getroot()\n",
        "    for c in root.iter('junction'):\n",
        "        node = c.attrib['id']\n",
        "        if c.attrib['type'] == 'internal' or not 'intersection' in node:\n",
        "            continue\n",
        "        arr.append(node)\n",
        "    return arr\n",
        "\n",
        "def get_intersection_neighborhoods(filepath):\n",
        "    tree = ET.parse(filepath)\n",
        "    root = tree.getroot()\n",
        "    dic = {}\n",
        "    for c in root.iter('junction'):\n",
        "        node = c.attrib['id']\n",
        "        if c.attrib['type'] == 'internal' or 'intersection' not in node:\n",
        "            continue\n",
        "        dic[node] = []\n",
        "        incLanes = c.attrib['incLanes']\n",
        "        incLanes_list = incLanes.split(' ')\n",
        "        for lane in incLanes_list:\n",
        "            lane_split = lane.split('___')[0]\n",
        "            if 'intersection' in lane_split:\n",
        "                dic[node].append(lane_split)\n",
        "    max_neighbors = max([len(v) for v in dic.values()]) if dic else 0\n",
        "    return dic, max_neighbors + 1\n",
        "\"\"\")\n",
        "\n",
        "with open(\"utils/utils.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "\n",
        "class Counter:\n",
        "    def __init__(self):\n",
        "        self.val = mp.Value(\"i\", 0)\n",
        "        self.lock = mp.Lock()\n",
        "    def get(self):\n",
        "        with self.lock: return self.val.value\n",
        "    def increment(self, amt=1):\n",
        "        with self.lock: self.val.value += amt\n",
        "\n",
        "def tensor(x, device):\n",
        "    if isinstance(x, torch.Tensor): return x\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    return torch.tensor(x, device=device, dtype=torch.float32)\n",
        "\n",
        "def random_sample(indices, batch_size):\n",
        "    indices = np.asarray(np.random.permutation(indices))\n",
        "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
        "    for batch in batches: yield batch\n",
        "    r = len(indices) % batch_size\n",
        "    if r: yield indices[-r:]\n",
        "\n",
        "def layer_init_filter(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def ensure_shared_grads(model, shared_model):\n",
        "    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
        "        if shared_param.grad is not None: return\n",
        "        shared_param._grad = param.grad\n",
        "\"\"\")\n",
        "print(\"âœ… Archivos de ayuda creados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkxsKk4ycg0S",
        "outputId": "9f4ad2db-3b56-497c-9e90-a5942c2b363a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Creando directorio 'utils'...\n",
            "âœ… Archivos de ayuda creados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 3: DEFINICIÃ“N DEL ENTORNO\n",
        "# ===================================================================\n",
        "import os\n",
        "os.makedirs('environments', exist_ok=True)\n",
        "print(\"ğŸ“‚ Creando directorio 'environments'...\")\n",
        "\n",
        "with open(\"environments/intersections.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from utils.net_scrape import get_intersections, get_intersection_neighborhoods\n",
        "\n",
        "# CONSTANTES GLOBALES\n",
        "PER_AGENT_STATE_SIZE = 4 # Jam_length(2) + Phase(1) + Elapsed(1)\n",
        "GLOBAL_STATE_SIZE = 1 # Sim_step\n",
        "ACTION_SIZE = 2 # NS o EW\n",
        "\n",
        "class SumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600, agent_id=\"default\"):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.agent_id = agent_id\n",
        "        self.label = str(agent_id)\n",
        "\n",
        "        self.intersections = get_intersections(self.net_file)\n",
        "        self.vehicles = dict()\n",
        "        self.traci = None\n",
        "\n",
        "        self._start_simulation()\n",
        "        self.lanes = self.traci.lane.getIDList()\n",
        "        self.edge_ids = self.traci.edge.getIDList()\n",
        "        self.intersection_lanes = {intr: self._get_lanes(intr) for intr in self.intersections}\n",
        "\n",
        "    def _get_lanes(self, intersection_id):\n",
        "        lanes = set()\n",
        "        for edge in self.edge_ids:\n",
        "            if edge.startswith(\":\"): continue\n",
        "            if self.traci.lane.getEdgeID(edge + \"_0\") == edge:\n",
        "                if self.traci.edge.getToJunction(edge) == intersection_id:\n",
        "                    lanes.add(edge + \"_0\")\n",
        "        return list(lanes)\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        sumo_cmd = [\n",
        "            sumo_binary,\n",
        "            '-n', self.net_file,\n",
        "            '-r', self.route_file,\n",
        "            '--time-to-teleport', '-1',\n",
        "            '--no-warnings', 'true'\n",
        "        ]\n",
        "        traci.start(sumo_cmd, label=self.label)\n",
        "        self.traci = traci.getConnection(self.label)\n",
        "\n",
        "    def reset(self):\n",
        "        if self.traci is not None:\n",
        "            self.traci.close()\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        state = [[] for _ in self.intersections]\n",
        "        sim_time = self.traci.simulation.getTime() / self.num_seconds\n",
        "\n",
        "        for i, intr in enumerate(self.intersections):\n",
        "            # Jam length por carril de entrada\n",
        "            jam_len = [self.traci.lane.getLastStepHaltingNumber(lane) for lane in self.intersection_lanes[intr]]\n",
        "            # Phase\n",
        "            phase = self.traci.trafficlight.getPhase(intr)\n",
        "            # Tiempo en la fase\n",
        "            elapsed = self.traci.trafficlight.getPhaseDuration(intr) - self.traci.trafficlight.getNextSwitch(intr) + sim_time*self.num_seconds\n",
        "\n",
        "            # Normalizar y agregar\n",
        "            state[i].extend([val / 10.0 for val in jam_len]) # Normaliza\n",
        "            state[i].append(phase / 3.0) # Normaliza\n",
        "            state[i].append(elapsed / 100.0) # Normaliza\n",
        "            state[i].append(sim_time)\n",
        "\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, intr in enumerate(self.intersections):\n",
        "            current_phase = self.traci.trafficlight.getPhase(intr)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0: # Solo cambia si es verde y la acciÃ³n es 1\n",
        "                self.traci.trafficlight.setPhase(intr, (current_phase + 1) % 4)\n",
        "\n",
        "        self.traci.simulationStep()\n",
        "\n",
        "        state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0\n",
        "\n",
        "        return state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward = -total_wait_time\n",
        "\n",
        "        rewards = np.array([reward / len(self.intersections)] * len(self.intersections))\n",
        "        return rewards\n",
        "\n",
        "    def close(self):\n",
        "        if self.traci:\n",
        "            self.traci.close()\n",
        "\"\"\")\n",
        "print(\"âœ… Entorno multi-agente creado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5scktFb8cpVX",
        "outputId": "478aa3c2-964c-40bd-f75b-edb93b20fbed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Creando directorio 'environments'...\n",
            "âœ… Entorno multi-agente creado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 4: MODELO PPO Y WORKER\n",
        "# ===================================================================\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('workers', exist_ok=True)\n",
        "print(\"ğŸ“‚ Creando directorios 'models' y 'workers'...\")\n",
        "\n",
        "# --- Modelo PPO (Actor-CrÃ­tico) ---\n",
        "with open(\"models/ppo_model.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from utils.utils import layer_init_filter\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # Red del Actor\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        # Red del CrÃ­tico\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.apply(layer_init_filter)\n",
        "\n",
        "    def forward(self, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        return action.detach(), action_logprob.detach()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\"\"\")\n",
        "\n",
        "# --- Worker (LÃ³gica de entrenamiento por agente) ---\n",
        "with open(\"workers/ppo_worker.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "from utils.utils import tensor\n",
        "from environments.intersections import SumoEnv\n",
        "\n",
        "class PPOWorker:\n",
        "    def __init__(self, agent_id, shared_model, optimizer, constants, device):\n",
        "        self.agent_id = agent_id\n",
        "        self.device = device\n",
        "        self.constants = constants\n",
        "        self.gamma = constants['ppo']['discount']\n",
        "        self.eps_clip = constants['ppo']['ppo_ratio_clip']\n",
        "        self.K_epochs = constants['ppo']['optimization_epochs']\n",
        "\n",
        "        self.env = SumoEnv(\n",
        "            net_file='data/2_2_intersections.net.xml',\n",
        "            route_file='data/2_2_intersections.rou.xml',\n",
        "            agent_id=f\"worker_{self.agent_id}\"\n",
        "        )\n",
        "\n",
        "        self.state_dim = self.env._get_state().shape[1]\n",
        "        self.action_dim = 2 # NS o EW\n",
        "\n",
        "        self.policy = shared_model\n",
        "        self.optimizer = optimizer\n",
        "        self.MseLoss = torch.nn.MSELoss()\n",
        "\n",
        "    def train(self, memory_buffer):\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory_buffer['rewards']), reversed(memory_buffer['is_terminals'])):\n",
        "            if is_terminal: discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        old_states = torch.squeeze(torch.stack(memory_buffer['states'], dim=0)).to(self.device).detach()\n",
        "        old_actions = torch.squeeze(torch.stack(memory_buffer['actions'], dim=0)).to(self.device).detach()\n",
        "        old_logprobs = torch.squeeze(torch.stack(memory_buffer['logprobs'], dim=0)).to(self.device).detach()\n",
        "\n",
        "        for _ in range(self.K_epochs):\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\"\"\")\n",
        "print(\"âœ… Modelo PPO y Worker creados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlqYETI4dQ-k",
        "outputId": "6c03df09-3570-4a33-a580-0cd4fdcb4259"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Creando directorios 'models' y 'workers'...\n",
            "âœ… Modelo PPO y Worker creados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 5: SCRIPT PRINCIPAL DE ENTRENAMIENTO\n",
        "# ===================================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "from models.ppo_model import ActorCritic\n",
        "from workers.ppo_worker import PPOWorker\n",
        "import time\n",
        "\n",
        "# --- HIPERPARÃMETROS ---\n",
        "CONSTANTS = {\n",
        "    \"ppo\": {\n",
        "        \"discount\": 0.99,\n",
        "        \"ppo_ratio_clip\": 0.2,\n",
        "        \"optimization_epochs\": 80,\n",
        "        \"learning_rate\": 0.0003\n",
        "    },\n",
        "    \"max_episodes\": 1000,\n",
        "    \"max_timesteps\": 1500,\n",
        "    \"update_timestep\": 4000\n",
        "}\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_AGENTS = 4 # Para una cuadrÃ­cula 2x2\n",
        "STATE_DIM = 6 # DimensiÃ³n del estado por agente (ajustar si cambia en el entorno)\n",
        "ACTION_DIM = 2 # 2 acciones posibles (NS, EW)\n",
        "HIDDEN_DIM = 64\n",
        "MODELO_GUARDADO = \"ppo_multi_agent_traffic.pth\"\n",
        "\n",
        "\n",
        "# --- LÃ“GICA DE ENTRENAMIENTO ---\n",
        "print(f\"ğŸš€ Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# Crear un modelo compartido y optimizador\n",
        "shared_model = ActorCritic(STATE_DIM, ACTION_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(shared_model.parameters(), lr=CONSTANTS[\"ppo\"][\"learning_rate\"])\n",
        "\n",
        "# BÃºfer de memoria para cada agente\n",
        "memory_buffers = [{'states': [], 'actions': [], 'logprobs': [], 'rewards': [], 'is_terminals': []} for _ in range(NUM_AGENTS)]\n",
        "workers = [PPOWorker(i, shared_model, optimizer, CONSTANTS, DEVICE) for i in range(NUM_AGENTS)]\n",
        "\n",
        "print(\"ğŸš¦ Inicializando workers y entorno...\")\n",
        "states = [worker.env.reset() for worker in workers]\n",
        "states = np.array(states).reshape(NUM_AGENTS, -1) # Aplanar estados\n",
        "\n",
        "timestep_count = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# Bucle principal de entrenamiento\n",
        "for i_episode in range(1, CONSTANTS[\"max_episodes\"] + 1):\n",
        "    for t in range(CONSTANTS[\"max_timesteps\"]):\n",
        "        timestep_count += 1\n",
        "\n",
        "        # Cada agente elige una acciÃ³n\n",
        "        actions = []\n",
        "        log_probs = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(NUM_AGENTS):\n",
        "                state_tensor = torch.FloatTensor(states[i]).to(DEVICE)\n",
        "                action, log_prob = shared_model.act(state_tensor)\n",
        "                actions.append(action)\n",
        "                log_probs.append(log_prob)\n",
        "\n",
        "        # Ejecutar acciÃ³n en cada entorno de worker y recolectar datos\n",
        "        next_states, rewards, dones = [], [], []\n",
        "        for i, worker in enumerate(workers):\n",
        "            # El step del entorno espera una lista de acciones\n",
        "            # AquÃ­ lo simplificamos a que cada worker maneje su entorno\n",
        "            # En una implementaciÃ³n real, se enviarÃ­a el vector `actions`\n",
        "            s_prime, r, done = worker.env.step([actions[i].item()])\n",
        "            next_states.append(s_prime)\n",
        "            rewards.append(r)\n",
        "            dones.append(done)\n",
        "\n",
        "            memory_buffers[i]['states'].append(torch.FloatTensor(states[i]))\n",
        "            memory_buffers[i]['actions'].append(actions[i])\n",
        "            memory_buffers[i]['logprobs'].append(log_probs[i])\n",
        "            memory_buffers[i]['rewards'].append(rewards[i][0]) # Tomar el reward escalar\n",
        "            memory_buffers[i]['is_terminals'].append(done)\n",
        "\n",
        "        states = np.array(next_states).reshape(NUM_AGENTS, -1)\n",
        "\n",
        "        # ActualizaciÃ³n de la polÃ­tica\n",
        "        if timestep_count % CONSTANTS[\"update_timestep\"] == 0:\n",
        "            print(f\"ğŸ§  Actualizando polÃ­tica en el timestep {timestep_count}...\")\n",
        "            for i in range(NUM_AGENTS):\n",
        "                workers[i].train(memory_buffers[i])\n",
        "                # Limpiar buffer despuÃ©s de actualizar\n",
        "                memory_buffers[i] = {'states': [], 'actions': [], 'logprobs': [], 'rewards': [], 'is_terminals': []}\n",
        "\n",
        "            timestep_count = 0\n",
        "\n",
        "        if any(dones):\n",
        "            break\n",
        "\n",
        "    # Guardar el modelo y mostrar progreso\n",
        "    if i_episode % 50 == 0:\n",
        "        print(f\"Episodio: {i_episode}, Recompensa promedio: {np.mean(rewards):.2f}, Tiempo: {time.time() - start_time:.2f}s\")\n",
        "        torch.save(shared_model.state_dict(), MODELO_GUARDADO)\n",
        "        print(f\"ğŸ’¾ Modelo guardado como '{MODELO_GUARDADO}'\")\n",
        "\n",
        "print(\"\\n\\n--- ENTRENAMIENTO FINALIZADO ---\")\n",
        "for worker in workers:\n",
        "    worker.env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "LN-N32oidVeQ",
        "outputId": "6617f8ba-6931-4d1f-e3b6-46a44d7f248e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/traci/__init__.py:42: UserWarning: Could not import libsumo using /usr/bin/python3, falling back to pure python traci (No module named 'libsumo').\n",
            "  warnings.warn(\"Could not import libsumo using %s, falling back to pure python traci (%s).\" %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Usando dispositivo: cpu\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1666632860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# BÃºfer de memoria para cada agente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmemory_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logprobs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rewards'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_terminals'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPPOWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONSTANTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸš¦ Inicializando workers y entorno...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-1666632860.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# BÃºfer de memoria para cada agente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmemory_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logprobs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rewards'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_terminals'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPPOWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONSTANTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸš¦ Inicializando workers y entorno...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/workers/ppo_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_id, shared_model, optimizer, constants, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# NS o EW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "agahWT6ggy_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xqY95DQWgzCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZhGXUQvgzF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 1: INSTALACIÃ“N\n",
        "# ===================================================================\n",
        "import os\n",
        "print(\"âš™ï¸ Instalando SUMO...\")\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo add-apt-repository -y ppa:sumo/stable > /dev/null\n",
        "!sudo apt-get update -y > /dev/null\n",
        "!sudo apt-get install -y sumo sumo-tools sumo-doc > /dev/null\n",
        "os.environ['SUMO_HOME'] = '/usr/share/sumo'\n",
        "print(\"âœ… SUMO instalado.\")\n",
        "print(\"\\nğŸ Instalando bibliotecas de RL...\")\n",
        "!pip install -q torch traci\n",
        "print(\"âœ… Bibliotecas de RL instaladas.\")\n",
        "\n",
        "# ===================================================================\n",
        "# CELDA 2: CREACIÃ“N DEL ESCENARIO 2x2\n",
        "# ===================================================================\n",
        "escenario_dir = 'mi_escenario_grid2x2'\n",
        "os.makedirs(escenario_dir, exist_ok=True)\n",
        "print(f\"ğŸ“‚ Creando escenario en '{escenario_dir}'...\")\n",
        "with open(f\"{escenario_dir}/grid.nod.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<nodes>\n",
        "    <node id=\"C11\" x=\"0\" y=\"100\" type=\"traffic_light\"/> <node id=\"C12\" x=\"100\" y=\"100\" type=\"traffic_light\"/>\n",
        "    <node id=\"C21\" x=\"0\" y=\"0\" type=\"traffic_light\"/>   <node id=\"C22\" x=\"100\" y=\"0\" type=\"traffic_light\"/>\n",
        "    <node id=\"N1\" x=\"0\" y=\"200\"/> <node id=\"N2\" x=\"100\" y=\"200\"/>\n",
        "    <node id=\"S1\" x=\"0\" y=\"-100\"/> <node id=\"S2\" x=\"100\" y=\"-100\"/>\n",
        "    <node id=\"W1\" x=\"-100\" y=\"100\"/> <node id=\"W2\" x=\"-100\" y=\"0\"/>\n",
        "    <node id=\"E1\" x=\"200\" y=\"100\"/> <node id=\"E2\" x=\"200\" y=\"0\"/>\n",
        "</nodes>\"\"\")\n",
        "with open(f\"{escenario_dir}/grid.edg.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<edges>\n",
        "    <edge from=\"W1\" to=\"C11\" id=\"W1_C11\"/> <edge from=\"C11\" to=\"W1\" id=\"C11_W1\"/>\n",
        "    <edge from=\"W2\" to=\"C21\" id=\"W2_C21\"/> <edge from=\"C21\" to=\"W2\" id=\"C21_W2\"/>\n",
        "    <edge from=\"E1\" to=\"C12\" id=\"E1_C12\"/> <edge from=\"C12\" to=\"E1\" id=\"C12_E1\"/>\n",
        "    <edge from=\"E2\" to=\"C22\" id=\"E2_C22\"/> <edge from=\"C22\" to=\"E2\" id=\"C22_E2\"/>\n",
        "    <edge from=\"N1\" to=\"C11\" id=\"N1_C11\"/> <edge from=\"C11\" to=\"N1\" id=\"C11_N1\"/>\n",
        "    <edge from=\"N2\" to=\"C12\" id=\"N2_C12\"/> <edge from=\"C12\" to=\"N2\" id=\"C12_N2\"/>\n",
        "    <edge from=\"S1\" to=\"C21\" id=\"S1_C21\"/> <edge from=\"C21\" to=\"S1\" id=\"C21_S1\"/>\n",
        "    <edge from=\"S2\" to=\"C22\" id=\"S2_C22\"/> <edge from=\"C22\" to=\"S2\" id=\"C22_S2\"/>\n",
        "    <edge from=\"C11\" to=\"C12\" id=\"C11_C12\"/> <edge from=\"C12\" to=\"C11\" id=\"C12_C11\"/>\n",
        "    <edge from=\"C21\" to=\"C22\" id=\"C21_C22\"/> <edge from=\"C22\" to=\"C21\" id=\"C22_C21\"/>\n",
        "    <edge from=\"C11\" to=\"C21\" id=\"C11_C21\"/> <edge from=\"C21\" to=\"C11\" id=\"C21_C11\"/>\n",
        "    <edge from=\"C12\" to=\"C22\" id=\"C12_C22\"/> <edge from=\"C22\" to=\"C12\" id=\"C22_C12\"/>\n",
        "</edges>\"\"\")\n",
        "!netconvert --node-files={escenario_dir}/grid.nod.xml --edge-files={escenario_dir}/grid.edg.xml --output-file={escenario_dir}/grid.net.xml > /dev/null\n",
        "with open(f\"{escenario_dir}/grid.rou.xml\", \"w\") as f:\n",
        "    f.write(\"\"\"<routes>\n",
        "    <vType id=\"car\" accel=\"2.6\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" maxSpeed=\"70\"/>\n",
        "    <flow id=\"f_we\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"W1_C11\" to=\"C12_E1\"/>\n",
        "    <flow id=\"f_ew\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"150\" from=\"E2_C22\" to=\"C21_W2\"/>\n",
        "    <flow id=\"f_ns\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"N1_C11\" to=\"C21_S1\"/>\n",
        "    <flow id=\"f_sn\" type=\"car\" begin=\"0\" end=\"3600\" vehsPerHour=\"200\" from=\"S2_C22\" to=\"C12_N2\"/>\n",
        "</routes>\"\"\")\n",
        "print(\"âœ… Escenario 2x2 creado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J2ww-rkgzR5",
        "outputId": "3432fc5c-2555-4c6a-e45a-071624e19287"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Instalando SUMO...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 21.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "âœ… SUMO instalado.\n",
            "\n",
            "ğŸ Instalando bibliotecas de RL...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Bibliotecas de RL instaladas.\n",
            "ğŸ“‚ Creando escenario en 'mi_escenario_grid2x2'...\n",
            "/bin/bash: line 1:  3517 Segmentation fault      (core dumped) netconvert --node-files=mi_escenario_grid2x2/grid.nod.xml --edge-files=mi_escenario_grid2x2/grid.edg.xml --output-file=mi_escenario_grid2x2/grid.net.xml > /dev/null\n",
            "âœ… Escenario 2x2 creado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 3: ENTORNO, MODELO Y ENTRENAMIENTO (VERSIÃ“N SIMPLIFICADA)\n",
        "# ===================================================================\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import time\n",
        "\n",
        "# --- 1. CLASE DE ENTORNO (SIMPLIFICADA) ---\n",
        "class MultiAgentSumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        self.ts_ids = []\n",
        "        self.lanes = []\n",
        "        self._start_simulation()\n",
        "        self.ts_ids = list(self.traci.trafficlight.getIDList())\n",
        "        self.lanes = list(self.traci.lane.getIDList())\n",
        "        self.state_dim = 5 # phase, avg_wait_time, avg_speed, density, queue\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        try:\n",
        "            traci.close()\n",
        "        except:\n",
        "            pass\n",
        "        traci.start([self.sumo_binary, \"-n\", self.net_file, \"-r\", self.route_file, \"--time-to-teleport\", \"-1\", \"--no-warnings\", \"true\"])\n",
        "        self.traci = traci\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        states = []\n",
        "        for ts_id in self.ts_ids:\n",
        "            phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            incoming_lanes = set()\n",
        "            for link in self.traci.trafficlight.getControlledLinks(ts_id):\n",
        "                incoming_lanes.add(link[0][0])\n",
        "\n",
        "            if not incoming_lanes:\n",
        "                states.append(np.zeros(self.state_dim))\n",
        "                continue\n",
        "\n",
        "            avg_wait_time = np.mean([self.traci.lane.getWaitingTime(lane) for lane in incoming_lanes])\n",
        "            avg_speed = np.mean([self.traci.lane.getLastStepMeanSpeed(lane) for lane in incoming_lanes])\n",
        "            density = np.mean([self.traci.lane.getLastStepOccupancy(lane) for lane in incoming_lanes])\n",
        "            queue = np.mean([self.traci.lane.getLastStepHaltingNumber(lane) for lane in incoming_lanes])\n",
        "\n",
        "            state = [phase/3.0, avg_wait_time/100.0, avg_speed/20.0, density, queue/20.0]\n",
        "            states.append(np.array(state, dtype=np.float32))\n",
        "        return np.array(states)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, ts_id in enumerate(self.ts_ids):\n",
        "            current_phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0:\n",
        "                self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n",
        "\n",
        "        self.traci.simulationStep()\n",
        "\n",
        "        next_state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0 or self.traci.simulation.getTime() >= self.num_seconds\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward_per_agent = -total_wait_time / len(self.ts_ids)\n",
        "        return np.array([reward_per_agent] * len(self.ts_ids))\n",
        "\n",
        "    def close(self):\n",
        "        traci.close()\n",
        "\n",
        "# --- 2. CLASE DEL MODELO ACTOR-CRÃTICO ---\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.detach(), dist.log_prob(action).detach()\n",
        "\n",
        "# --- 3. LÃ“GICA DE ENTRENAMIENTO ---\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸš€ Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# --- HiperparÃ¡metros ---\n",
        "TOTAL_TIMESTEPS = 20000\n",
        "TIMESTEPS_POR_ACTUALIZACION = 2048\n",
        "LR = 0.002\n",
        "GAMMA = 0.99\n",
        "\n",
        "# --- InicializaciÃ³n ---\n",
        "env = MultiAgentSumoEnv(net_file='mi_escenario_grid2x2/grid.net.xml', route_file='mi_escenario_grid2x2/grid.rou.xml')\n",
        "state_dim = env.state_dim\n",
        "action_dim = 2 # 0: quedarse, 1: cambiar\n",
        "num_agents = len(env.ts_ids)\n",
        "\n",
        "policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=LR)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# --- Bucle de entrenamiento ---\n",
        "timestep_count = 0\n",
        "while timestep_count < TOTAL_TIMESTEPS:\n",
        "    states, log_probs, rewards, dones = [], [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    # Recolectar un batch de experiencia\n",
        "    for t in range(TIMESTEPS_POR_ACTUALIZACION):\n",
        "        timestep_count += 1\n",
        "\n",
        "        # Actuar\n",
        "        state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "        actions, log_prob = policy.act(state_tensor)\n",
        "        next_state, reward, done = env.step(actions.cpu().numpy())\n",
        "\n",
        "        # Guardar experiencia\n",
        "        states.append(state_tensor)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(torch.FloatTensor(reward).to(DEVICE))\n",
        "        dones.append(torch.FloatTensor([done] * num_agents).to(DEVICE))\n",
        "\n",
        "        state = next_state\n",
        "        if done: break\n",
        "\n",
        "    print(f\"Timestep: {timestep_count}/{TOTAL_TIMESTEPS}, Recompensa del Ãºltimo paso: {np.mean(reward):.3f}\")\n",
        "\n",
        "    # Calcular recompensas descontadas (rewards-to-go)\n",
        "    discounted_rewards = []\n",
        "    cumulative_reward = torch.zeros(num_agents).to(DEVICE)\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        cumulative_reward = reward + GAMMA * cumulative_reward * (1 - done)\n",
        "        discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "    # Actualizar la polÃ­tica\n",
        "    states = torch.stack(states).to(DEVICE).view(-1, state_dim)\n",
        "    old_log_probs = torch.stack(log_probs).to(DEVICE).view(-1)\n",
        "    discounted_rewards = torch.stack(discounted_rewards).to(DEVICE).view(-1)\n",
        "\n",
        "    # EvaluaciÃ³n del valor del estado\n",
        "    state_values = policy.critic(states).squeeze()\n",
        "\n",
        "    # Calcular la pÃ©rdida y optimizar\n",
        "    advantages = discounted_rewards - state_values.detach()\n",
        "    actor_loss = -(advantages * old_log_probs).mean()\n",
        "    critic_loss = mse_loss(state_values, discounted_rewards)\n",
        "\n",
        "    loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# --- Limpieza y guardado final ---\n",
        "torch.save(policy.state_dict(), 'ppo_multi_agente_final_20k.pth')\n",
        "env.close()\n",
        "print(\"\\n--- âœ… ENTRENAMIENTO FINALIZADO. Modelo guardado como 'ppo_multi_agente_final.pth' ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjJiqD69g71w",
        "outputId": "be25beda-92a7-4186-b39b-1faf37471b70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Usando dispositivo: cuda\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1547248210.py:64: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
            "  self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestep: 2048/20000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 4096/20000, Recompensa del Ãºltimo paso: -0.500\n",
            " Retrying in 1 seconds\n",
            "Timestep: 6144/20000, Recompensa del Ãºltimo paso: -1.750\n",
            " Retrying in 1 seconds\n",
            "Timestep: 8192/20000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 10240/20000, Recompensa del Ãºltimo paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 12288/20000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 14336/20000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 16384/20000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 18432/20000, Recompensa del Ãºltimo paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 20480/20000, Recompensa del Ãºltimo paso: 0.000\n",
            "\n",
            "--- âœ… ENTRENAMIENTO FINALIZADO. Modelo guardado como 'ppo_multi_agente_final.pth' ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 4: MÃ“DULO DE EVALUACIÃ“N Y COMPARACIÃ“N\n",
        "# ===================================================================\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "print(\"--- ğŸ“Š Iniciando EvaluaciÃ³n Comparativa ---\")\n",
        "\n",
        "# --- PARÃMETROS ---\n",
        "ESCENARIO_DIR = 'mi_escenario_grid2x2'\n",
        "MODELO_A_CARGAR = 'ppo_multi_agente_final_20k.pth'\n",
        "ARCHIVO_RESULTADOS_CSV = f\"20k_resultados_comparativos_{MODELO_A_CARGAR.replace('.pth', '')}.csv\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# --- CLASES NECESARIAS (Copiadas de la celda de entrenamiento) ---\n",
        "# Necesitamos redefinir las clases aquÃ­ para que la celda sea autÃ³noma.\n",
        "class MultiAgentSumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600, additional_sumo_cmd=None):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.additional_sumo_cmd = additional_sumo_cmd or []\n",
        "        self.sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        self._start_simulation()\n",
        "        self.ts_ids = list(self.traci.trafficlight.getIDList())\n",
        "        self.lanes = list(self.traci.lane.getIDList())\n",
        "        self.state_dim = 5\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        try: traci.close()\n",
        "        except: pass\n",
        "        sumo_cmd = [self.sumo_binary, \"-n\", self.net_file, \"-r\", self.route_file, \"--time-to-teleport\", \"-1\", \"--no-warnings\", \"true\"]\n",
        "        sumo_cmd.extend(self.additional_sumo_cmd)\n",
        "        traci.start(sumo_cmd)\n",
        "        self.traci = traci\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        states = []\n",
        "        for ts_id in self.ts_ids:\n",
        "            phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            incoming_lanes = {link[0][0] for link in self.traci.trafficlight.getControlledLinks(ts_id)}\n",
        "            if not incoming_lanes:\n",
        "                states.append(np.zeros(self.state_dim))\n",
        "                continue\n",
        "            avg_wait_time = np.mean([self.traci.lane.getWaitingTime(lane) for lane in incoming_lanes])\n",
        "            avg_speed = np.mean([self.traci.lane.getLastStepMeanSpeed(lane) for lane in incoming_lanes])\n",
        "            density = np.mean([self.traci.lane.getLastStepOccupancy(lane) for lane in incoming_lanes])\n",
        "            queue = np.mean([self.traci.lane.getLastStepHaltingNumber(lane) for lane in incoming_lanes])\n",
        "            state = [phase/3.0, avg_wait_time/100.0, avg_speed/20.0, density, queue/20.0]\n",
        "            states.append(np.array(state, dtype=np.float32))\n",
        "        return np.array(states)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, ts_id in enumerate(self.ts_ids):\n",
        "            current_phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0:\n",
        "                self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n",
        "        self.traci.simulationStep()\n",
        "        next_state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0 or self.traci.simulation.getTime() >= self.num_seconds\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward_per_agent = -total_wait_time / len(self.ts_ids)\n",
        "        return np.array([reward_per_agent] * len(self.ts_ids))\n",
        "\n",
        "    def close(self):\n",
        "        traci.close()\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1))\n",
        "        self.critic = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.detach(), dist.log_prob(action).detach()\n",
        "\n",
        "# --- 1. Simular y Grabar el Caso Base (Tiempos Fijos) ---\n",
        "print(\"\\n1/3: ğŸš¦ Ejecutando simulaciÃ³n del Caso Base (tiempos fijos)...\")\n",
        "sumo_cmd_base = [\n",
        "    sumolib.checkBinary('sumo'),\n",
        "    \"-n\", f\"{ESCENARIO_DIR}/grid.net.xml\",\n",
        "    \"-r\", f\"{ESCENARIO_DIR}/grid.rou.xml\",\n",
        "    \"--tripinfo-output\", f\"{ESCENARIO_DIR}/tripinfo_base.xml\",\n",
        "    \"--time-to-teleport\", \"-1\",\n",
        "    \"--no-warnings\", \"true\"\n",
        "]\n",
        "traci.start(sumo_cmd_base)\n",
        "while traci.simulation.getMinExpectedNumber() > 0:\n",
        "    traci.simulationStep()\n",
        "traci.close()\n",
        "\n",
        "# --- 2. Simular y Grabar el Agente RL Entrenado ---\n",
        "print(\"2/3: ğŸ¤– Ejecutando simulaciÃ³n del Agente RL...\")\n",
        "# Cargar el modelo entrenado\n",
        "env_eval = MultiAgentSumoEnv(\n",
        "    net_file=f'{ESCENARIO_DIR}/grid.net.xml',\n",
        "    route_file=f'{ESCENARIO_DIR}/grid.rou.xml',\n",
        "    additional_sumo_cmd=[\"--tripinfo-output\", f\"{ESCENARIO_DIR}/tripinfo_entrenado.xml\"]\n",
        ")\n",
        "state_dim = env_eval.state_dim\n",
        "action_dim = 2\n",
        "policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "policy.load_state_dict(torch.load(MODELO_A_CARGAR, map_location=DEVICE))\n",
        "policy.eval() # Poner en modo de evaluaciÃ³n\n",
        "\n",
        "state = env_eval.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        actions, _ = policy.act(state_tensor)\n",
        "    state, _, done = env_eval.step(actions.cpu().numpy())\n",
        "env_eval.close()\n",
        "print(\"âœ… Simulaciones completadas.\")\n",
        "\n",
        "# --- 3. Parsear y Analizar los Resultados ---\n",
        "print(\"3/3: ğŸ“ˆ Analizando resultados...\")\n",
        "def parse_tripinfo(xml_file):\n",
        "    try:\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        trips = [{'duration': float(t.get('duration')), 'timeLoss': float(t.get('timeLoss'))} for t in root.findall('tripinfo')]\n",
        "        return pd.DataFrame(trips)\n",
        "    except (FileNotFoundError, ET.ParseError):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "df_base = parse_tripinfo(f'{ESCENARIO_DIR}/tripinfo_base.xml')\n",
        "df_entrenado = parse_tripinfo(f'{ESCENARIO_DIR}/tripinfo_entrenado.xml')\n",
        "\n",
        "if not df_base.empty and not df_entrenado.empty:\n",
        "    avg_base = df_base.mean()\n",
        "    avg_entrenado = df_entrenado.mean()\n",
        "    mejora = ((avg_base - avg_entrenado) / avg_base) * 100\n",
        "    df_resultados = pd.DataFrame({\"Caso Base (Fijo)\": avg_base, \"Agente RL\": avg_entrenado, \"Mejora (%)\": mejora})\n",
        "    df_resultados.index = [\"DuraciÃ³n Media del Viaje (s)\", \"Tiempo Medio Perdido (s)\"]\n",
        "\n",
        "    print(\"\\n\\n--- ANÃLISIS DE RENDIMIENTO ---\")\n",
        "    print(df_resultados.to_string(formatters={'Mejora (%)': '{:,.2f}%'.format}))\n",
        "\n",
        "    df_resultados.to_csv(ARCHIVO_RESULTADOS_CSV)\n",
        "    print(f\"\\n\\n--- âœ… Proceso de evaluaciÃ³n completado. Resultados guardados en '{ARCHIVO_RESULTADOS_CSV}' ---\")\n",
        "else:\n",
        "    print(\"\\nâŒ No se pudieron generar las mÃ©tricas. AsegÃºrate de que ambas simulaciones se ejecutaron correctamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy_6emG5hAYg",
        "outputId": "21da1a59-197c-4d98-8cfd-8eae0a25364d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ğŸ“Š Iniciando EvaluaciÃ³n Comparativa ---\n",
            "\n",
            "1/3: ğŸš¦ Ejecutando simulaciÃ³n del Caso Base (tiempos fijos)...\n",
            " Retrying in 1 seconds\n",
            "2/3: ğŸ¤– Ejecutando simulaciÃ³n del Agente RL...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2701626409.py:70: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
            "  self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Simulaciones completadas.\n",
            "3/3: ğŸ“ˆ Analizando resultados...\n",
            "\n",
            "\n",
            "--- ANÃLISIS DE RENDIMIENTO ---\n",
            "                              Caso Base (Fijo)  Agente RL Mejora (%)\n",
            "DuraciÃ³n Media del Viaje (s)         44.712857  34.317986     23.25%\n",
            "Tiempo Medio Perdido (s)             22.999129  12.589237     45.26%\n",
            "\n",
            "\n",
            "--- âœ… Proceso de evaluaciÃ³n completado. Resultados guardados en '20k_resultados_comparativos_ppo_multi_agente_final_20k.csv' ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELDA 3: ENTRENAMIENTO CON GRÃFICA DE PROGRESO (CORREGIDA)\n",
        "# ===================================================================\n",
        "import traci\n",
        "import sumolib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import time\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. CLASE DE ENTORNO (SIN CAMBIOS) ---\n",
        "class MultiAgentSumoEnv:\n",
        "    def __init__(self, net_file, route_file, use_gui=False, num_seconds=3600, additional_sumo_cmd=None):\n",
        "        self.net_file = net_file\n",
        "        self.route_file = route_file\n",
        "        self.use_gui = use_gui\n",
        "        self.num_seconds = num_seconds\n",
        "        self.additional_sumo_cmd = additional_sumo_cmd or []\n",
        "        self.sumo_binary = sumolib.checkBinary('sumo-gui' if self.use_gui else 'sumo')\n",
        "        self._start_simulation()\n",
        "        self.ts_ids = list(self.traci.trafficlight.getIDList())\n",
        "        self.lanes = list(self.traci.lane.getIDList())\n",
        "        self.state_dim = 5\n",
        "\n",
        "    def _start_simulation(self):\n",
        "        try: traci.close()\n",
        "        except: pass\n",
        "        sumo_cmd = [self.sumo_binary, \"-n\", self.net_file, \"-r\", self.route_file, \"--time-to-teleport\", \"-1\", \"--no-warnings\", \"true\"]\n",
        "        sumo_cmd.extend(self.additional_sumo_cmd)\n",
        "        traci.start(sumo_cmd)\n",
        "        self.traci = traci\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        states = []\n",
        "        for ts_id in self.ts_ids:\n",
        "            phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            incoming_lanes = {link[0][0] for link in self.traci.trafficlight.getControlledLinks(ts_id)}\n",
        "            if not incoming_lanes:\n",
        "                states.append(np.zeros(self.state_dim))\n",
        "                continue\n",
        "            avg_wait_time = np.mean([self.traci.lane.getWaitingTime(lane) for lane in incoming_lanes])\n",
        "            avg_speed = np.mean([self.traci.lane.getLastStepMeanSpeed(lane) for lane in incoming_lanes])\n",
        "            density = np.mean([self.traci.lane.getLastStepOccupancy(lane) for lane in incoming_lanes])\n",
        "            queue = np.mean([self.traci.lane.getLastStepHaltingNumber(lane) for lane in incoming_lanes])\n",
        "            state = [phase/3.0, avg_wait_time/100.0, avg_speed/20.0, density, queue/20.0]\n",
        "            states.append(np.array(state, dtype=np.float32))\n",
        "        return np.array(states)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for i, ts_id in enumerate(self.ts_ids):\n",
        "            current_phase = self.traci.trafficlight.getPhase(ts_id)\n",
        "            if actions[i] == 1 and current_phase % 2 == 0:\n",
        "                self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n",
        "        self.traci.simulationStep()\n",
        "        next_state = self._get_state()\n",
        "        reward = self._get_reward()\n",
        "        done = self.traci.simulation.getMinExpectedNumber() == 0 or self.traci.simulation.getTime() >= self.num_seconds\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _get_reward(self):\n",
        "        total_wait_time = sum(self.traci.lane.getWaitingTime(lane) for lane in self.lanes)\n",
        "        reward_per_agent = -total_wait_time / len(self.ts_ids) if len(self.ts_ids) > 0 else 0\n",
        "        return np.array([reward_per_agent] * len(self.ts_ids))\n",
        "\n",
        "    def close(self):\n",
        "        traci.close()\n",
        "\n",
        "# --- 2. CLASE DEL MODELO ACTOR-CRÃTICO (SIN CAMBIOS) ---\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1))\n",
        "        self.critic = nn.Sequential(nn.Linear(state_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.detach(), dist.log_prob(action).detach()\n",
        "\n",
        "# --- NUEVO: FunciÃ³n de evaluaciÃ³n periÃ³dica ---\n",
        "def evaluate_policy(policy, net_file, route_file, eval_seconds=1800, device='cpu'):\n",
        "    print(f\"--- ğŸ“ˆ Evaluando rendimiento...\")\n",
        "    eval_env = MultiAgentSumoEnv(\n",
        "        net_file=net_file,\n",
        "        route_file=route_file,\n",
        "        num_seconds=eval_seconds,\n",
        "        additional_sumo_cmd=[\"--tripinfo-output\", \"tripinfo_eval.xml\"]\n",
        "    )\n",
        "    state = eval_env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state).to(device)\n",
        "        with torch.no_grad():\n",
        "            actions, _ = policy.act(state_tensor)\n",
        "        state, _, done = eval_env.step(actions.cpu().numpy())\n",
        "    eval_env.close()\n",
        "\n",
        "    try:\n",
        "        tree = ET.parse(\"tripinfo_eval.xml\")\n",
        "        root = tree.getroot()\n",
        "        times = [float(t.get('timeLoss')) for t in root.findall('tripinfo')]\n",
        "        return np.mean(times) if times else 0\n",
        "    except (FileNotFoundError, ET.ParseError):\n",
        "        return None\n",
        "\n",
        "# --- 3. LÃ“GICA DE ENTRENAMIENTO (CON CORRECCIONES) ---\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸš€ Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# --- HiperparÃ¡metros ---\n",
        "TOTAL_TIMESTEPS = 25000\n",
        "TIMESTEPS_POR_ACTUALIZACION = 2048\n",
        "EVAL_INTERVAL = 5000\n",
        "LR = 0.002\n",
        "GAMMA = 0.99\n",
        "\n",
        "# --- InicializaciÃ³n ---\n",
        "env = MultiAgentSumoEnv(net_file='mi_escenario_grid2x2/grid.net.xml', route_file='mi_escenario_grid2x2/grid.rou.xml')\n",
        "state_dim = env.state_dim\n",
        "action_dim = 2\n",
        "num_agents = len(env.ts_ids)\n",
        "policy = ActorCritic(state_dim, action_dim).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=LR)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# --- Listas para guardar el historial para la grÃ¡fica ---\n",
        "history_timesteps = []\n",
        "history_waiting_time = []\n",
        "last_eval_timestep = 0\n",
        "\n",
        "# --- Bucle de entrenamiento ---\n",
        "timestep_count = 0\n",
        "while timestep_count < TOTAL_TIMESTEPS:\n",
        "    states, log_probs, rewards, dones = [], [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(TIMESTEPS_POR_ACTUALIZACION):\n",
        "        timestep_count += 1\n",
        "        state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "        actions, log_prob = policy.act(state_tensor)\n",
        "        next_state, reward, done = env.step(actions.cpu().numpy())\n",
        "        states.append(state_tensor)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(torch.FloatTensor(reward).to(DEVICE))\n",
        "        dones.append(torch.FloatTensor([done] * num_agents).to(DEVICE))\n",
        "        state = next_state\n",
        "        if done: break\n",
        "\n",
        "    print(f\"Timestep: {timestep_count}/{TOTAL_TIMESTEPS}, Recompensa del Ãºltimo paso: {np.mean(reward):.3f}\")\n",
        "\n",
        "    # --- Bloque de aprendizaje (CON LÃNEAS CORREGIDAS) ---\n",
        "    discounted_rewards = []\n",
        "    cumulative_reward = torch.zeros(num_agents).to(DEVICE)\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        cumulative_reward = reward + GAMMA * cumulative_reward * (1 - done)\n",
        "        discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "    # --- CORRECCIÃ“N AQUÃ: Se reemplazÃ³ `-_1` por `-1` ---\n",
        "    states = torch.stack(states).to(DEVICE).view(-1, state_dim)\n",
        "    old_log_probs = torch.stack(log_probs).to(DEVICE).view(-1)\n",
        "    discounted_rewards = torch.stack(discounted_rewards).to(DEVICE).view(-1)\n",
        "\n",
        "    state_values = policy.critic(states).squeeze()\n",
        "    advantages = discounted_rewards - state_values.detach()\n",
        "    actor_loss = -(advantages * old_log_probs).mean()\n",
        "    critic_loss = mse_loss(state_values, discounted_rewards)\n",
        "    loss = actor_loss + 0.5 * critic_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- Bloque de evaluaciÃ³n periÃ³dica ---\n",
        "    if timestep_count - last_eval_timestep >= EVAL_INTERVAL:\n",
        "        avg_wait_time = evaluate_policy(policy, 'mi_escenario_grid2x2/grid.net.xml', 'mi_escenario_grid2x2/grid.rou.xml', device=DEVICE)\n",
        "        if avg_wait_time is not None:\n",
        "            history_timesteps.append(timestep_count)\n",
        "            history_waiting_time.append(avg_wait_time)\n",
        "            print(f\"--- Resultado EvaluaciÃ³n: {avg_wait_time:.2f}s de tiempo de espera promedio ---\\n\")\n",
        "        last_eval_timestep = timestep_count\n",
        "\n",
        "# --- Limpieza y guardado final ---\n",
        "torch.save(policy.state_dict(), 'ppo_multi_agente_final.pth')\n",
        "env.close()\n",
        "print(\"\\n--- âœ… ENTRENAMIENTO FINALIZADO ---\")\n",
        "\n",
        "# --- Graficar el progreso del aprendizaje ---\n",
        "if history_timesteps:\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.plot(history_timesteps, history_waiting_time, marker='o', linestyle='-')\n",
        "    plt.title('Progreso del Entrenamiento: Tiempo de Espera vs. Timesteps', fontsize=16)\n",
        "    plt.xlabel('Timesteps de Entrenamiento', fontsize=12)\n",
        "    plt.ylabel('Tiempo de Espera Promedio (s)', fontsize=12)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcAJKoAg60Wo",
        "outputId": "c532c0f2-9584-4b8e-8ce7-21d433bf54c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Usando dispositivo: cuda\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2956219106.py:61: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
            "  self.traci.trafficlight.setPhase(ts_id, (current_phase + 1) % len(self.traci.trafficlight.getCompleteRedYellowGreenDefinition(ts_id)[0].phases))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestep: 2048/25000, Recompensa del Ãºltimo paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 4096/25000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 6144/25000, Recompensa del Ãºltimo paso: -0.250\n",
            "--- ğŸ“ˆ Evaluando rendimiento...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n",
            "--- Resultado EvaluaciÃ³n: 13.57s de tiempo de espera promedio ---\n",
            "\n",
            " Retrying in 1 seconds\n",
            "Timestep: 8192/25000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 10240/25000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 12288/25000, Recompensa del Ãºltimo paso: -0.250\n",
            "--- ğŸ“ˆ Evaluando rendimiento...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n",
            "--- Resultado EvaluaciÃ³n: 13.38s de tiempo de espera promedio ---\n",
            "\n",
            " Retrying in 1 seconds\n",
            "Timestep: 14336/25000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n",
            "Timestep: 16384/25000, Recompensa del Ãºltimo paso: -0.250\n",
            " Retrying in 1 seconds\n",
            "Timestep: 18432/25000, Recompensa del Ãºltimo paso: 0.000\n",
            "--- ğŸ“ˆ Evaluando rendimiento...\n",
            " Retrying in 1 seconds\n",
            " Retrying in 1 seconds\n",
            "--- Resultado EvaluaciÃ³n: 13.28s de tiempo de espera promedio ---\n",
            "\n",
            " Retrying in 1 seconds\n",
            "Timestep: 20480/25000, Recompensa del Ãºltimo paso: 0.000\n",
            " Retrying in 1 seconds\n"
          ]
        }
      ]
    }
  ]
}